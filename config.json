{
    "script": "train_flux_lora_ui_kontext_new.py",
    "seed": 4321,
    "mixed_precision": "bf16",
    "report_to": "wandb",
    "lr_warmup_steps": 0,
    "output_dir": "F:\\models\\qwen\\wlop_demo",
    "save_name": "wlop_demo",
    "train_data_dir": "F:\\ImageSet\\Anime\\qwen_demo",
    "optimizer": "adamw",
    "lr_scheduler": "constant",
    "learning_rate": 0.0001,
    "train_batch_size": 1,
    "repeats": 10,
    "gradient_accumulation_steps": 1,
    "num_train_epochs": 10,
    "save_model_epochs": 1,
    "validation_epochs": 1,
    "rank": 16,
    "skip_epoch": 0,
    "skip_step": 0,
    "gradient_checkpointing": true,
    "validation_ratio": 0.1,
    "pretrained_model_name_or_path": "F:\\T2ITrainer\\flux_models\\kontext",
    "model_path": "",
    "resume_from_checkpoint": "",
    "recreate_cache": false,
    "config_path": "config.json",
    "resolution": "512",
    "caption_dropout": 0.1,
    "cosine_restarts": 1,
    "max_time_steps": 1000,
    "blocks_to_swap": 0,
    "mask_dropout": 1,
    "reg_ratio": 0.5,
    "reg_timestep": 900,
    "use_two_captions": false,
    "slider_positive_scale": 1,
    "slider_negative_scale": -1,
    "image_configs": {
        "train": {
            "suffix": ""
        }
    },
    "caption_configs": {
        "train": {
            "ext": ".txt"
        }
    },
    "training_set": [
        {
            "training_layout_configs": {
                "train": {
                    "target": "train",
                    "noised": true
                }
            },
            "captions_selection": {
                "target": "train"
            }
        }
    ]
}