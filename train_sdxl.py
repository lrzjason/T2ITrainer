#!/usr/bin/env python
# coding=utf-8
# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# this is a practice codebase, mainly inspired from diffusers train_text_to_image_sdxl.py
# this codebase mainly to get the training working rather than many option to set
# therefore, it would assume something like fp16 vae fixed and baked in model, etc
# some option, me doesn't used in training wouldn't implemented like ema, etc

# before 20240401, initial codebase
# 20240401, added caption ext support
#           need to handle multi batch size and gas for dataset stack images.
#           need to implement validation loss
#           need to add te training
# 20240402 bucketing works!!!!, many thanks to @minienglish1 from everydream discord
#          added whole repeats to dataset
from diffusers.models.attention_processor import AttnProcessor2_0
import jsonlines

import argparse
import functools
import gc
import logging
import math
import os
import random
import shutil
from pathlib import Path

import accelerate
import datasets
import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
import transformers
import diffusers


from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration, set_seed
from datasets import load_dataset
from packaging import version
from torchvision import transforms
from torchvision.transforms.functional import crop
from tqdm.auto import tqdm
from transformers import AutoTokenizer, PretrainedConfig
from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    DiffusionPipeline,
    UNet2DConditionModel,
)
from pathlib import Path
from diffusers.optimization import get_scheduler
from diffusers.training_utils import EMAModel, compute_snr
from diffusers.utils import check_min_version, is_wandb_available
from diffusers.utils.import_utils import is_xformers_available
from diffusers.utils.torch_utils import is_compiled_module

from diffusers import StableDiffusionXLPipeline
from tqdm import tqdm 
from PIL import Image 
from compel import Compel, ReturnedEmbeddingsType

from sklearn.model_selection import train_test_split


import json


import sys
sys.path.append('F:/T2ITrainer/utils')

import image_utils
from image_utils import BucketBatchSampler, CachedImageDataset

# from meta
# https://github.com/facebookresearch/schedule_free
import schedulefree

# Will error if the minimal version of diffusers is not installed. Remove at your own risks.
check_min_version("0.27.0.dev0")

logger = get_logger(__name__)

DATASET_NAME_MAPPING = {
    "lambdalabs/pokemon-blip-captions": ("image", "text"),
}

def compute_embeddings(batch, vae, compel, proportion_empty_prompts, caption_column,recreate=False):
    path = batch.pop("path")[0]
    full_path = os.path.join(args.train_data_dir, f"{path}.npz")
    # check if file exists
    if os.path.exists(full_path):
        # load file via torch
        try:
            if recreate:
                # remove the cache
                os.remove(full_path)
            else:
                embedding = torch.load(full_path)
                return embedding
        except Exception as e:
            print(e)
            print(f"{full_path} is corrupted, regenerating...")
    

    images = batch.pop("pixel_values")
    pixel_values = torch.stack(list(images))
    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
    # print('vae.dtype',vae.dtype)
    pixel_values = pixel_values.to(vae.device, dtype=vae.dtype)

    with torch.no_grad():
        model_input = vae.encode(pixel_values).latent_dist.sample()
    model_input = model_input * vae.config.scaling_factor

    prompt = batch[caption_column][0] if isinstance(batch[caption_column][0], str) else ""
    if random.random() < proportion_empty_prompts:
        prompt = ""
    prompt_embeds, pooled_prompt_embeds = compel(prompt)
    
    latent = {
        "model_input": model_input.cpu(),
        "prompt_embeds": prompt_embeds.cpu(), 
        "pooled_prompt_embeds": pooled_prompt_embeds.cpu()
    }
    # save latent to cache file
    torch.save(latent, full_path)
    # print(f"Saved latent to {full_path}")
    return latent


def generate_timestep_weights(args, num_timesteps):
    weights = torch.ones(num_timesteps)

    # Determine the indices to bias
    num_to_bias = int(args.timestep_bias_portion * num_timesteps)

    if args.timestep_bias_strategy == "later":
        bias_indices = slice(-num_to_bias, None)
    elif args.timestep_bias_strategy == "earlier":
        bias_indices = slice(0, num_to_bias)
    elif args.timestep_bias_strategy == "range":
        # Out of the possible 1000 timesteps, we might want to focus on eg. 200-500.
        range_begin = args.timestep_bias_begin
        range_end = args.timestep_bias_end
        if range_begin < 0:
            raise ValueError(
                "When using the range strategy for timestep bias, you must provide a beginning timestep greater or equal to zero."
            )
        if range_end > num_timesteps:
            raise ValueError(
                "When using the range strategy for timestep bias, you must provide an ending timestep smaller than the number of timesteps."
            )
        bias_indices = slice(range_begin, range_end)
    else:  # 'none' or any other string
        return weights
    if args.timestep_bias_multiplier <= 0:
        return ValueError(
            "The parameter --timestep_bias_multiplier is not intended to be used to disable the training of specific timesteps."
            " If it was intended to disable timestep bias, use `--timestep_bias_strategy none` instead."
            " A timestep bias multiplier less than or equal to 0 is not allowed."
        )

    # Apply the bias
    weights[bias_indices] *= args.timestep_bias_multiplier

    # Normalize
    weights /= weights.sum()

    return weights


def parse_args(input_args=None):
    parser = argparse.ArgumentParser(description="Practice of a training script.")
    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
    
    parser.add_argument(
        "--output_dir",
        type=str,
        default="diffusion-dpo-lora",
        help="The output directory where the model predictions and checkpoints will be written.",
    )
    parser.add_argument(
        "--logging_dir",
        type=str,
        default="logs",
        help=(
            "[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to"
            " *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***."
        ),
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default=None,
        required=True,
        help="Single file .ckpt or .safetensors model file",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    parser.add_argument(
        "--mixed_precision",
        type=str,
        default=None,
        choices=["no", "fp16", "bf16"],
        help=(
            "Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
            " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
            " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
        ),
    )
    parser.add_argument(
        "--allow_tf32",
        action="store_true",
        help=(
            "Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see"
            " https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices"
        ),
    )
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        help="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.",
    )
    parser.add_argument(
        "--report_to",
        type=str,
        default="tensorboard",
        help=(
            'The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
            ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
        ),
    )
    parser.add_argument(
        "--enable_xformers_memory_efficient_attention", action="store_true", help="Whether or not to use xformers."
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument(
        "--scale_lr",
        action="store_true",
        default=False,
        help="Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.",
    )

    
    parser.add_argument(
        "--use_8bit_adam", action="store_true", help="Whether or not to use 8-bit Adam from bitsandbytes."
    )

    
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        help=(
            "The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,"
            " dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,"
            " or to a folder containing files that ðŸ¤— Datasets can understand."
        ),
    )
    parser.add_argument(
        "--dataset_config_name",
        type=str,
        default=None,
        help="The config of the Dataset, leave as None if there's only one config.",
    )
    parser.add_argument(
        "--train_data_dir",
        type=str,
        default=None,
        help=(
            "A folder containing the training data. Folder contents must follow the structure described in"
            " https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file"
            " must exist to provide the captions for the images. Ignored if `dataset_name` is specified."
        ),
    )
    # parser.add_argument(
    #     "--image_column", type=str, default="image", help="The column of the dataset containing an image."
    # )
    # parser.add_argument(
    #     "--caption_column",
    #     type=str,
    #     default="text",
    #     help="The column of the dataset containing a caption or a list of captions.",
    # )
    parser.add_argument("--adam_beta1", type=float, default=0.9, help="The beta1 parameter for the Adam optimizer.")
    parser.add_argument("--adam_beta2", type=float, default=0.999, help="The beta2 parameter for the Adam optimizer.")
    parser.add_argument("--adam_weight_decay", type=float, default=1e-2, help="Weight decay to use.")
    parser.add_argument("--adam_epsilon", type=float, default=1e-08, help="Epsilon value for the Adam optimizer")
    

    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="The directory where the downloaded models and datasets will be stored.",
    )

    parser.add_argument(
        "--resolution",
        type=int,
        default=1024,
        help=(
            "The resolution for input images, all the images in the train/validation dataset will be resized to this"
            " resolution"
        ),
    )

    
    parser.add_argument(
        "--center_crop",
        default=False,
        action="store_true",
        help=(
            "Whether to center crop the input images to the resolution. If not set, the images will be randomly"
            " cropped. The images will be resized to the resolution first before cropping."
        ),
    )
    
    parser.add_argument(
        "--max_train_samples",
        type=int,
        default=None,
        help=(
            "For debugging purposes or quicker training, truncate the number of training examples to this "
            "value if set."
        ),
    )
    parser.add_argument(
        "--proportion_empty_prompts",
        type=float,
        default=0,
        help="Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).",
    )

    parser.add_argument(
        "--train_batch_size", type=int, default=1, help="Batch size (per device) for the training dataloader."
    )

    
    parser.add_argument(
        "--dataloader_num_workers",
        type=int,
        default=0,
        help=(
            "Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process."
        ),
    )

    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="Total number of training steps to perform.  If provided, overrides num_train_epochs.",
    )
    parser.add_argument(
        "--lr_scheduler",
        type=str,
        default="constant",
        help=(
            'The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
            ' "constant", "constant_with_warmup"]'
        ),
    )

    
    parser.add_argument(
        "--lr_warmup_steps", type=int, default=500, help="Number of steps for the warmup in the lr scheduler."
    )

    parser.add_argument("--num_train_epochs", type=int, default=20)
    
    
    parser.add_argument(
        "--timestep_bias_strategy",
        type=str,
        default="none",
        choices=["earlier", "later", "range", "none"],
        help=(
            "The timestep bias strategy, which may help direct the model toward learning low or high frequency details."
            " Choices: ['earlier', 'later', 'range', 'none']."
            " The default is 'none', which means no bias is applied, and training proceeds normally."
            " The value of 'later' will increase the frequency of the model's final training timesteps."
        ),
    )
    parser.add_argument(
        "--timestep_bias_multiplier",
        type=float,
        default=1.0,
        help=(
            "The multiplier for the bias. Defaults to 1.0, which means no bias is applied."
            " A value of 2.0 will double the weight of the bias, and a value of 0.5 will halve it."
        ),
    )
    parser.add_argument(
        "--timestep_bias_begin",
        type=int,
        default=0,
        help=(
            "When using `--timestep_bias_strategy=range`, the beginning (inclusive) timestep to bias."
            " Defaults to zero, which equates to having no specific bias."
        ),
    )
    parser.add_argument(
        "--timestep_bias_end",
        type=int,
        default=1000,
        help=(
            "When using `--timestep_bias_strategy=range`, the final timestep (inclusive) to bias."
            " Defaults to 1000, which is the number of timesteps that Stable Diffusion is trained on."
        ),
    )
    parser.add_argument(
        "--timestep_bias_portion",
        type=float,
        default=0.25,
        help=(
            "The portion of timesteps to bias. Defaults to 0.25, which 25% of timesteps will be biased."
            " A value of 0.5 will bias one half of the timesteps. The value provided for `--timestep_bias_strategy` determines"
            " whether the biased portions are in the earlier or later timesteps."
        ),
    )
    parser.add_argument("--max_grad_norm", default=1.0, type=float, help="Max gradient norm.")
    
    parser.add_argument(
        "--save_name",
        type=str,
        default="test",
        help=(
            'Save model name start with this'
        ),
    )
    parser.add_argument(
        "--checkpointing_steps",
        type=int,
        default=500,
        help=(
            "Save a checkpoint of the training state every X updates. These checkpoints can be used both as final"
            " checkpoints in case they are better than the last checkpoint, and are also suitable for resuming"
            " training using `--resume_from_checkpoint`." 
        ),
    )


    parser.add_argument(
        "--validation_prompt",
        type=str,
        default=None,
        help="A prompt that is used during validation to verify that the model is learning.",
    )
    parser.add_argument(
        "--num_validation_images",
        type=int,
        default=1,
        help="Number of images that should be generated during validation with `validation_prompt`.",
    )
    parser.add_argument(
        "--validation_epochs",
        type=int,
        default=1,
        help=(
            "Run fine-tuning validation every X epochs. The validation process consists of running the prompt"
            " `args.validation_prompt` multiple times: `args.num_validation_images`."
        ),
    )
    parser.add_argument(
        "--caption_exts",
        type=str,
        default=".txt",
        help=(
            "Run fine-tuning validation every X epochs. The validation process consists of running the prompt"
            " `args.validation_prompt` multiple times: `args.num_validation_images`."
        ),
    )
    parser.add_argument(
        "--validation_ratio",
        type=float,
        default=0.1,
        help=(
            "validation_ratio split ratio of validation set for val/loss"
        ),
    )

    

    
    if input_args is not None:
        args = parser.parse_args(input_args)
    else:
        args = parser.parse_args()

    env_local_rank = int(os.environ.get("LOCAL_RANK", -1))
    if env_local_rank != -1 and env_local_rank != args.local_rank:
        args.local_rank = env_local_rank

    # Sanity checks
    if args.dataset_name is None and args.train_data_dir is None:
        raise ValueError("Need either a dataset name or a training folder.")

    if args.proportion_empty_prompts < 0 or args.proportion_empty_prompts > 1:
        raise ValueError("`--proportion_empty_prompts` must be in the range [0, 1].")

    return args



# training main function
def main(args):
    args.seed = 4321
    args.output_dir = 'F:/models/unet/output'
    args.logging_dir = 'logs'
    args.model_path = 'F:/models/Stable-diffusion/sdxl/o2/openxl2_008.safetensors'
    args.mixed_precision = "fp16"
    # args.report_to = "tensorboard"
    
    args.report_to = "wandb"
    args.enable_xformers_memory_efficient_attention = True
    args.gradient_checkpointing = True
    args.allow_tf32 = True


    # args.train_data_dir = 'F:/ImageSet/openxl2_realism'
    # try to use clip filtered dataset
    args.train_data_dir = 'F:/ImageSet/openxl2_realism_above_average'
    args.num_train_epochs = 60
    args.lr_warmup_steps = 1
    # reduce lr from 1e-5 to 2e-6
    args.learning_rate = 1.2e-6
    args.train_batch_size = 2
    # reduce gas from 500 to 100
    args.gradient_accumulation_steps = 100
    # increase save steps from 50 to 250
    args.checkpointing_steps = 90
    args.resume_from_checkpoint = ""
    # args.resume_from_checkpoint = "F:/models/unet/output/actual_run-50"
    args.save_name = "openxl2_b9"
    args.lr_scheduler = "constant"
    # args.lr_scheduler = "cosine"

    
    # args.train_data_dir = 'F:/ImageSet/openxl2_realism_test'
    # args.resume_from_checkpoint = "F:/models/unet/output/test_run-50"
    # args.num_train_epochs = 2
    # args.train_batch_size = 1
    # args.gradient_accumulation_steps = 1
    # args.save_name = "test_run"

    args.scale_lr = False
    args.use_8bit_adam = True
    args.adam_beta1 = 0.9
    # args.adam_beta2 = 0.999
    args.adam_beta2 = 0.99

    args.adam_weight_decay = 1e-2
    args.adam_epsilon = 1e-08
    # args.train_data_dir = 'F:/ImageSet/training_script_testset_sdxl_train_validation/train/test'
    # args.train_data_dir = 'F:/ImageSet/training_script_cotton_doll/train'
    # args.train_data_dir = 'F:/ImageSet/openxl2_realism_test'
    args.dataset_name = None
    args.cache_dir = None
    args.caption_column = None
    # args.image_column = 'image'
    # args.caption_column = 'text'
    args.resolution = 1024
    args.center_crop = False
    args.max_train_samples = None
    args.proportion_empty_prompts = 0
    args.dataloader_num_workers = 0
    args.max_train_steps = None

    args.timestep_bias_portion = 0.25
    args.timestep_bias_end = 1000
    args.timestep_bias_begin = 0
    args.timestep_bias_multiplier = 1.0
    args.timestep_bias_strategy = "none"
    args.max_grad_norm = 1.0
    args.validation_prompt = "cosplay photo, A female character in a unique outfit, holding two large, serrated weapons. The character has silver hair, wears a blue dress with black accents, and has a white flower accessory in her hair. The background is minimalistic, featuring a white floor and a few white flowers. The composition is dynamic, with the character positioned in a mid-action pose, and the perspective is from a frontal angle, emphasizing the character's stature and the weapons she wields. 1girl, kaine_(nier), solo, weapon, dual_wielding, underwear, bandages, high_heels, holding, flower, white_hair, gloves, sword, white_panties, breasts, panties, negligee, bandaged_leg, full_body, holding_weapon, hair_ornament, bandaged_arm, lingerie, thigh_strap, hair_flower, holding_sword, "
    
    args.validation_epochs = 1
    args.validation_ratio = 0.1
    args.num_validation_images = 1
    args.caption_exts = '.txt,.wd14_cap'

    vae_path = "F:/models/VAE/sdxl_vae.safetensors"

    if args.report_to == "wandb":
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")
        import wandb
    # Make one log on every process with the configuration for debugging.
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    
    logging_dir = Path(args.output_dir, args.logging_dir)
    # create accelerator
    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)

    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
    )
    
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()
    
    set_seed(args.seed)


    # Handle the repository creation
    if accelerator.is_main_process:
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

    # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision
    # as these weights are only used for inference, keeping weights in full precision is not required.
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16

    pipeline = StableDiffusionXLPipeline.from_single_file(
    args.model_path,variant=weight_dtype, use_safetensors=True, 
    torch_dtype=weight_dtype).to(accelerator.device)
    # print('pipeline:')
    # print(pipeline)

    text_encoder_one = pipeline.text_encoder
    text_encoder_two = pipeline.text_encoder_2
    vae = pipeline.vae
    
    # vae = AutoencoderKL.from_single_file(
    #     vae_path
    # )
    unet = pipeline.unet
    # print(type(unet))

    # Load scheduler and models
    # from kohya ss sdxl_train.py 
    noise_scheduler = DDPMScheduler(
        beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000, clip_sample=False
    )

    # reference from kohya ss custom_train_functions.py
    def prepare_scheduler_for_custom_training(noise_scheduler, device):
        if hasattr(noise_scheduler, "all_snr"):
            return

        alphas_cumprod = noise_scheduler.alphas_cumprod
        sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
        sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)
        alpha = sqrt_alphas_cumprod
        sigma = sqrt_one_minus_alphas_cumprod
        all_snr = (alpha / sigma) ** 2

        noise_scheduler.all_snr = all_snr.to(device)
    
    prepare_scheduler_for_custom_training(noise_scheduler,accelerator.device)
    
    # Freeze vae 
    vae.requires_grad_(False)
    text_encoder_one.requires_grad_(False)
    text_encoder_two.requires_grad_(False)

    # Move unet and text_encoders to device and cast to weight_dtype
    unet.to(accelerator.device, dtype=weight_dtype)
    unet = accelerator.prepare(unet)
    # Assume fp16 vae got fixed and baked in model
    # exception handling would be added afterward
    # vae.to(accelerator.device, dtype=weight_dtype)
    vae.to(accelerator.device, dtype=torch.float32)
    text_encoder_one.to(accelerator.device, dtype=weight_dtype)
    text_encoder_two.to(accelerator.device, dtype=weight_dtype)
    
    # Set unet as trainable.
    unet.train()

    if accelerator.mixed_precision == "fp16":
        # from kohya_ss train_util
        org_unscale_grads = accelerator.scaler._unscale_grads_
        def _unscale_grads_replacer(optimizer, inv_scale, found_inf, allow_fp16):
            return org_unscale_grads(optimizer, inv_scale, found_inf, True)

        accelerator.scaler._unscale_grads_ = _unscale_grads_replacer


    # ================================================================================
    # Memory_efficient
    # ================================================================================
    # if args.enable_xformers_memory_efficient_attention:
    #     if is_xformers_available():
    #         import xformers

    #         xformers_version = version.parse(xformers.__version__)
    #         if xformers_version == version.parse("0.0.16"):
    #             logger.warn(
    #                 "xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details."
    #             )
    #         unet.enable_xformers_memory_efficient_attention()
    #     else:
    #         raise ValueError("xformers is not available. Make sure it is installed correctly")
    print("Enable SDPA for U-Net")
    # unet.set_use_sdpa(True)
    unet.set_attn_processor(AttnProcessor2_0())
    if args.enable_xformers_memory_efficient_attention:
        if is_xformers_available():
            if torch.__version__ >= "2.0.0":  # PyTorch 2.0.0 ä»¥ä¸Šå¯¾å¿œã®xformersãªã‚‰ä»¥ä¸‹ãŒä½¿ãˆã‚‹
                vae.set_use_memory_efficient_attention_xformers(True)
    # ================================================================================
    # End Memory_efficient
    # ================================================================================

    # `accelerate` 0.16.0 will have better support for customized saving
    if version.parse(accelerate.__version__) >= version.parse("0.16.0"):
        # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format
        def save_model_hook(models, weights, output_dir):
            if accelerator.is_main_process:
                for i, model in enumerate(models):
                    model.save_pretrained(os.path.join(output_dir, "unet"))

                    # make sure to pop weight so that corresponding model is not saved again
                    weights.pop()

        def load_model_hook(models, input_dir):
            for i in range(len(models)):
                # pop models so that they are not loaded again
                model = models.pop()

                # load diffusers style into model
                load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder="unet")
                model.register_to_config(**load_model.config)

                model.load_state_dict(load_model.state_dict())
                del load_model

        accelerator.register_save_state_pre_hook(save_model_hook)
        accelerator.register_load_state_pre_hook(load_model_hook)
    
    if args.gradient_checkpointing:
        unet.enable_gradient_checkpointing()

    # Enable TF32 for faster training on Ampere GPUs,
    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
    if args.allow_tf32:
        torch.backends.cuda.matmul.allow_tf32 = True

    if args.scale_lr:
        args.learning_rate = (
            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes
        )

    # Use paged 8-bit Adam for more lower memory usage or to fine-tune the model
    # if args.use_8bit_adam:
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            raise ImportError(
                "To use paged 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`."
            )

        optimizer_class = bnb.optim.AdamW8bit
        # optimizer_class = bnb.optim.PagedAdamW8bit
    else:
        optimizer_class = torch.optim.AdamW

    

    # ==========================================================
    # Create train dataset
    # ==========================================================
    # data_files = {}
    # this part need more work afterward, you need to prepare 
    # the train files and val files split first before the training
    if args.train_data_dir is not None:
        # data_files["train"] = os.path.join(args.train_data_dir, "**")
        
        datarows = []
        # def read_caption(folder,filename,caption_ext=".txt"):
        #     # assume the caption file is the same as the image file
        #     # and ext is .txt
        #     # caption_file = file.replace('.jpg', '.txt')
        #     prompt = open(os.path.join(folder, f'{filename}{caption_ext}'), encoding='utf-8').read()
        #     prompt = prompt.replace('\n', ' ')
        #     dirname = os.path.basename(folder)
        #     # return f'{{"file_name": "{dirname}/{file}","text":"{prompt}","path": "{dirname}/{filename}"}}\n'
        #     return {
        #         "file_name": f"{dirname}/{file}", "text": prompt, "path": f"{dirname}/{filename}"
        #     }

        

        # create metadata.jsonl if not exist
        metadata_path = os.path.join(args.train_data_dir, 'metadata.json')
        if not os.path.exists(metadata_path):
            # metadata_file = open(metadata_path, 'w', encoding='utf-8')
            # for item in os.listdir(args.train_data_dir):
            #     # check item is dir or file
            #     item_path = os.path.join(args.train_data_dir, item)
            #     if os.path.isdir(item_path):
            #         folder_path = item_path
            #         for file in os.listdir(folder_path):
            #             if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.webp'):
            #                 # get filename and ext from file
            #                 filename, _ = os.path.splitext(file)
            #                 for ext in args.caption_exts.split(','):
            #                     if os.path.exists(os.path.join(folder_path, f'{filename}{ext}')):
            #                         # metadata_file.write(read_caption(folder_path,filename,ext))
            #                         caption = read_caption(folder_path,filename,ext)
            #                         datarows.append(caption)
            #     else:
            #         if item.endswith('.jpg') or item.endswith('.png') or item.endswith('.webp'):
            #             filename, _ = os.path.splitext(item)
            #             # metadata_file.write(read_caption(folder_path,filename))
            #             for ext in args.caption_exts.split(','):
            #                 if os.path.exists(os.path.join(folder_path, f'{filename}{ext}')):
            #                     # metadata_file.write(read_caption(folder_path,filename,ext))
            #                     caption = read_caption(folder_path,filename,ext)
            #                     datarows.append(caption)
            
            # using compel for longer prompt embedding
            compel = Compel(tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2] , text_encoder=[text_encoder_one, text_encoder_two], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])

            # create metadata and latent cache
            datarows = image_utils.create_metadata_cache(compel,vae,args.train_data_dir)
            # Serializing json
            json_object = json.dumps(datarows, indent=4)
            # Writing to sample.json
            with open(metadata_path, "w", encoding='utf-8') as outfile:
                outfile.write(json_object)
        else:
            with open(metadata_path, "r", encoding='utf-8') as readfile:
                datarows = json.loads(readfile.read())

    validation_datarows = []
    # prepare validation_slipt
    if args.validation_ratio > 0:
        # buckets = image_utils.get_buckets()
        train_ratio = 1 - args.validation_ratio
        validation_ratio = args.validation_ratio
        training_datarows, validation_datarows = train_test_split(datarows, train_size=train_ratio, test_size=validation_ratio)
        datarows = training_datarows


    # lazy implement of repeats
    datarows_clone = datarows.copy()
    # use epoch rather than repeats for more validation
    repeats = 1
    # repeats is 10, i in range(repeats) would execute 11 times
    for i in range(repeats-1):
        datarows = datarows + datarows_clone.copy()

    # clear memory 
    del text_encoder_one, text_encoder_two, pipeline.tokenizer, pipeline.tokenizer_2, vae
    gc.collect()
    torch.cuda.empty_cache()

    # ================================================================
    # End create embedding 
    # ================================================================
    
    def collate_fn(examples):
        # not sure if this would have issue when using multiple aspect ratio
        latents = torch.stack([example["latent"] for example in examples])
        time_ids = torch.stack([example["time_id"] for example in examples])
        prompt_embeds = torch.stack([example["prompt_embed"] for example in examples])
        pooled_prompt_embeds = torch.stack([example["pooled_prompt_embed"] for example in examples])

        return {
            "latents": latents,
            "prompt_embeds": prompt_embeds,
            "pooled_prompt_embeds": pooled_prompt_embeds,
            "time_ids": time_ids,
        }
    
    # create dataset based on input_dir
    train_dataset = CachedImageDataset(datarows,conditional_dropout_percent=0.1)

    # referenced from everyDream discord minienglish1 shared script
    #create bucket batch sampler
    bucket_batch_sampler = BucketBatchSampler(train_dataset, batch_size=args.train_batch_size, drop_last=True)

    #initialize the DataLoader with the bucket batch sampler
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_sampler=bucket_batch_sampler, #use bucket_batch_sampler instead of shuffle
        collate_fn=collate_fn,
        num_workers=args.dataloader_num_workers,
    )
    

    # Scheduler and math around the number of training steps.
    overrode_max_train_steps = False
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    # Optimizer creation
    params_to_optimize = unet.parameters()
    optimizer = optimizer_class(
        params_to_optimize,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )
    # not use, exceed vram
    # optimizer = schedulefree.AdamWScheduleFree(params_to_optimize, 
    #                 lr=args.learning_rate,
    #                 betas=(args.adam_beta1, args.adam_beta2),
    #                 weight_decay=args.adam_weight_decay,
    #                 eps=args.adam_epsilon
    #             )
    
    # try schedulefree fro meta
    # https://github.com/facebookresearch/schedule_free
    # optimizer.train()

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
    )

    # Prepare everything with our `accelerator`.
    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        unet, optimizer, train_dataloader, lr_scheduler
    )
    
    # unet, optimizer, train_dataloader = accelerator.prepare(
    #     unet, optimizer, train_dataloader
    # )

    # We need to recalculate our total training steps as the size of the training dataloader may have changed.
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    # Afterwards we recalculate our number of training epochs
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # We need to initialize the trackers we use, and also store our configuration.
    # The trackers initializes automatically on the main process.
    if accelerator.is_main_process:
        accelerator.init_trackers("text2image-fine-tune-sdxl", config=vars(args))

    # Train!
    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    global_step = 0
    first_epoch = 0


    # Potentially load in the weights and states from a previous save
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint != "latest":
            path = os.path.basename(args.resume_from_checkpoint)
        else:
            # Get the most recent checkpoint
            dirs = os.listdir(args.output_dir)
            dirs = [d for d in dirs if d.startswith("checkpoint")]
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
            path = dirs[-1] if len(dirs) > 0 else None

        if path is None:
            accelerator.print(
                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
            )
            args.resume_from_checkpoint = None
            initial_global_step = 0
        else:
            accelerator.print(f"Resuming from checkpoint {path}")
            accelerator.load_state(os.path.join(args.output_dir, path))
            global_step = int(path.split("-")[1])

            initial_global_step = global_step
            first_epoch = global_step // num_update_steps_per_epoch

    else:
        initial_global_step = 0
    # initial_global_step = 0



    progress_bar = tqdm(
        range(0, args.max_train_steps),
        initial=initial_global_step,
        desc="Steps",
        # Only show the progress bar once on each machine.
        disable=not accelerator.is_local_main_process,
    )

    # print("before epoch start")
    for epoch in range(first_epoch, args.num_train_epochs):
        train_loss = 0.0
        for step, batch in enumerate(train_dataloader):
            # https://github.com/facebookresearch/schedule_free/blob/main/examples/mnist/main.py#L39
            # optimizer.zero_grad()
            # print("loop dataloader")
            with accelerator.accumulate(unet):
                optimizer.zero_grad()
                # Sample noise that we'll add to the latents
                # model_input is vae encoded image aka latent
                latents = batch["latents"].to(accelerator.device)
                # get latent like random noise
                noise = torch.randn_like(latents)

                bsz = latents.shape[0]

                if args.timestep_bias_strategy == "none":
                    # Sample a random timestep for each image without bias.
                    # get 0~1000 random timestep
                    timesteps = torch.randint(
                        0, noise_scheduler.config.num_train_timesteps, (bsz,), device=accelerator.device
                    )
                else:
                    # Sample a random timestep for each image, potentially biased by the timestep weights.
                    # Biasing the timestep weights allows us to spend less time training irrelevant timesteps.
                    weights = generate_timestep_weights(args, noise_scheduler.config.num_train_timesteps).to(
                        accelerator.device
                    )
                    timesteps = torch.multinomial(weights, bsz, replacement=True).long()
                
                # Add noise to the model input according to the noise magnitude at each timestep
                # (this is the forward diffusion process)
                # model input is the encoded image latent
                # here is doing the forward diffusion process to add noise to latent and achieve the 
                # specific timesteps of latent noise
                noisy_model_input = noise_scheduler.add_noise(latents, noise, timesteps)

                add_time_ids = torch.cat(
                    [
                        batch["time_ids"].to(accelerator.device, dtype=weight_dtype)
                    ]
                )
                unet_added_conditions = {"time_ids": add_time_ids}
                prompt_embeds = batch["prompt_embeds"].to(accelerator.device)
                pooled_prompt_embeds = batch["pooled_prompt_embeds"].to(accelerator.device)
                unet_added_conditions.update({"text_embeds": pooled_prompt_embeds})

                with accelerator.autocast():
                    model_pred = unet(
                        noisy_model_input,
                        timesteps,
                        prompt_embeds,
                        added_cond_kwargs=unet_added_conditions,
                        return_dict=False,
                    )[0]

                # # Get the target for loss depending on the prediction type
                # if args.prediction_type is not None:
                #     # set prediction_type of scheduler if defined
                #     noise_scheduler.register_to_config(prediction_type=args.prediction_type)

                # if noise_scheduler.config.prediction_type == "epsilon":
                #     target = noise
                # elif noise_scheduler.config.prediction_type == "v_prediction":
                #     target = noise_scheduler.get_velocity(model_input, noise, timesteps)
                # elif noise_scheduler.config.prediction_type == "sample":
                #     # We set the target to latents here, but the model_pred will return the noise sample prediction.
                #     target = model_input
                #     # We will have to subtract the noise residual from the prediction to get the target sample.
                #     model_pred = model_pred - noise
                # else:
                #     raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}")

                # For the simplicity, only use epsilon
                target = noise

                # For the simplicity, only use mse_loss.
                # other option would implemented afterward, like debias
                # loss = F.nll_loss(model_pred.float(), target)
                # loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")


                #####################################################################
                # debiased estimation implementation
                # from kohya ss
                #####################################################################
                # do not mean over batch dimension for snr weight or scale v-pred loss
                # loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction="none")
                # loss = loss.mean([1, 2, 3])
                # # reference from kohya ss debias
                # def apply_debiased_estimation(loss, timesteps, noise_scheduler):
                #     snr_t = torch.stack([noise_scheduler.all_snr[t] for t in timesteps])  # batch_size
                #     snr_t = torch.minimum(snr_t, torch.ones_like(snr_t) * 1000)  # if timestep is 0, snr_t is inf, so limit it to 1000
                #     weight = 1/torch.sqrt(snr_t)
                #     loss = weight * loss
                #     return loss
                
                # loss = apply_debiased_estimation(loss, timesteps, noise_scheduler)
                # loss = loss.mean()  # mean over batch dimension
                #####################################################################
                # End debiased estimation implementation section
                #####################################################################

                
                #####################################################################
                # loss_huber implementation section
                #####################################################################
                # need to do a,b,a and b test on huber loss
                # loss_mse = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
                # loss_mse = loss
                loss_huber = F.huber_loss(model_pred.float(), target.float(), reduction="mean", delta=1.5)
                # loss = loss_mse + loss_huber
                loss = loss_huber
                #####################################################################
                # End loss_huber implementation section
                #####################################################################
                
                # Gather the losses across all processes for logging (if we use distributed training).
                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
                # train_loss += avg_loss.item() / args.gradient_accumulation_steps
                # args.gradient_accumulation_steps
                # due to 'with accelerator.accumulate(unet):' doesn't need to handle ' / args.gradient_accumulation_steps'
                # https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation 
                train_loss += avg_loss.item()

                
                # Backpropagate
                accelerator.backward(loss)
                # optimizer.step()

                # if accelerator.sync_gradients:
                #     params_to_clip = unet.parameters()
                #     accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                
                # args.gradient_accumulation_steps
                # https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation 
                optimizer.step()
                if accelerator.sync_gradients:
                    if not accelerator.optimizer_step_was_skipped:
                        lr_scheduler.step()
                optimizer.zero_grad()

                
            # Checks if the accelerator has performed an optimization step behind the scenes
            #post batch check for gradient updates
            accelerator.wait_for_everyone()
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                accelerator.log({"train_loss": train_loss / args.gradient_accumulation_steps}, step=global_step)
                train_loss = 0.0
                if accelerator.is_main_process:
                    if global_step % args.checkpointing_steps == 0:
                        save_path = os.path.join(args.output_dir, f"{args.save_name}-{global_step}")
                        accelerator.save_state(save_path)
                        logger.info(f"Saved state to {save_path}")

            
            logs = {"step_loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            # logs = {"step_loss": loss.detach().item(), "lr": args.learning_rate}
            progress_bar.set_postfix(**logs)
            
            if global_step >= args.max_train_steps:
                break
            
            del latents, noise, timesteps, add_time_ids, prompt_embeds, pooled_prompt_embeds, unet_added_conditions
            gc.collect()
            torch.cuda.empty_cache()
            
        # ==================================================
        # validation part
        # ==================================================
        if accelerator.is_main_process:
            if epoch % args.validation_epochs == 0:
                print('epoch',epoch)
                print('args.validation_epochs',args.validation_epochs)
                print('epoch _ args.validation_epochs',epoch % args.validation_epochs)
                if len(validation_datarows)>0:
                    validation_dataset = CachedImageDataset(validation_datarows,conditional_dropout_percent=0)
                    
                    val_batch_sampler = BucketBatchSampler(validation_dataset, batch_size=args.train_batch_size, drop_last=True)

                    #initialize the DataLoader with the bucket batch sampler
                    val_dataloader = torch.utils.data.DataLoader(
                        validation_dataset,
                        batch_sampler=val_batch_sampler, #use bucket_batch_sampler instead of shuffle
                        collate_fn=collate_fn,
                        num_workers=args.dataloader_num_workers,
                    )

                    print("beginning loss_validation")
                    
                    loss_validation_epoch = []
                    with torch.no_grad():
                        # basically the as same as the training loop
                        for i, batch in enumerate(val_dataloader):
                            # Sample noise that we'll add to the latents
                            # model_input is vae encoded image aka latent
                            latents = batch["latents"].to(accelerator.device)
                            # get latent like random noise
                            noise = torch.randn_like(latents)

                            bsz = latents.shape[0]

                            timesteps = torch.randint(
                                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=accelerator.device
                                )

                            noisy_model_input = noise_scheduler.add_noise(latents, noise, timesteps)

                            add_time_ids = torch.cat(
                                [
                                    batch["time_ids"].to(accelerator.device, dtype=weight_dtype)
                                ]
                            )
                            unet_added_conditions = {"time_ids": add_time_ids}
                            prompt_embeds = batch["prompt_embeds"].to(accelerator.device)
                            pooled_prompt_embeds = batch["pooled_prompt_embeds"].to(accelerator.device)
                            unet_added_conditions.update({"text_embeds": pooled_prompt_embeds})

                            with accelerator.autocast():
                                model_pred = unet(
                                    noisy_model_input,
                                    timesteps,
                                    prompt_embeds,
                                    added_cond_kwargs=unet_added_conditions,
                                    return_dict=False,
                                )[0]
                            
                            del latents,bsz,timesteps,add_time_ids, pooled_prompt_embeds, noisy_model_input,  prompt_embeds, unet_added_conditions

                            # For the simplicity, only use epsilon
                            target = noise

                            # For the simplicity, only use mse_loss.
                            # other option would implemented afterward, like debias
                            loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")

                            
                            del target, model_pred

                            loss_step = loss.detach().item()
                            loss_validation_epoch.append(loss_step)

                        if len(loss_validation_epoch) > 0:
                            avg_mean_loss = sum(loss_validation_epoch) / len(loss_validation_epoch)

                            accelerator.log({"loss/val": avg_mean_loss}, step=global_step)
                            # add lr to log
                            accelerator.log({"lr": lr_scheduler.get_last_lr()[0]}, step=global_step)
                            # print(f"loss/val:{avg_mean_loss}  global_step:{global_step}")
                            print(f"loss/val:{avg_mean_loss}  global_step:{global_step}  lr:{lr_scheduler.get_last_lr()[0]}")


                if args.validation_prompt is not None:
                    
                    print('args.validation_prompt',args.validation_prompt)
                    logger.info(
                        f"Running validation... /n Generating {args.num_validation_images} images with prompt:"
                        f" {args.validation_prompt}."
                    )
                    # create pipeline
                    
                    vae = AutoencoderKL.from_single_file(
                        vae_path
                    )
                    validation_pipeline = StableDiffusionXLPipeline.from_single_file(
                        args.model_path,variant=weight_dtype, use_safetensors=True, 
                        vae=vae,
                        unet=unet)
                    # if args.prediction_type is not None:
                    #     scheduler_args = {"prediction_type": args.prediction_type}
                    #     pipeline.scheduler = pipeline.scheduler.from_config(pipeline.scheduler.config, **scheduler_args)

                    validation_pipeline = validation_pipeline.to(accelerator.device, dtype=weight_dtype)
                    validation_pipeline.set_progress_bar_config(disable=True)
                    steps = 50
                    
                    compel = Compel(tokenizer=[validation_pipeline.tokenizer, validation_pipeline.tokenizer_2] , text_encoder=[validation_pipeline.text_encoder, validation_pipeline.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])

                    conditioning, pooled = compel(args.validation_prompt)
                    # run inference
                    # generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
                    # pipeline_args = {"prompt": args.validation_prompt}

                    with torch.cuda.amp.autocast():
                        images = validation_pipeline(prompt_embeds=conditioning, 
                                        pooled_prompt_embeds=pooled, 
                                        num_inference_steps=steps,
                                        guidance_scale=7,
                                        width=args.resolution, 
                                        height=args.resolution
                                        ).images

                    for tracker in accelerator.trackers:
                        if tracker.name == "tensorboard":
                            np_images = np.stack([np.asarray(img) for img in images])
                            tracker.writer.add_images("validation", np_images, epoch, dataformats="NHWC")
                            del np_images
                        if tracker.name == "wandb":
                            tracker.log(
                                {
                                    "validation": [
                                        wandb.Image(image, caption=f"{i}: {args.validation_prompt}")
                                        for i, image in enumerate(images)
                                    ]
                                }
                            )

                    del validation_pipeline,vae,compel
                    del images
                gc.collect()
                torch.cuda.empty_cache()
        # ==================================================
        # end validation part
        # ==================================================
    
    accelerator.wait_for_everyone()
    # ==================================================
    # validation part after training
    # ==================================================
    if accelerator.is_main_process:
        # save model
        save_path = os.path.join(args.output_dir, f"{args.save_name}-{global_step}")
        accelerator.save_state(save_path)

        del pipeline,noise_scheduler
        gc.collect()
        torch.cuda.empty_cache()

    # ==================================================
    # end validation part after training
    # ==================================================

    accelerator.end_training()
        #         break
        # break


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    args = parser.parse_args()
    main(args)