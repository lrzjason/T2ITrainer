import gradio as gr

import subprocess
import json
import sys
import os


default_config = {
    "script": "train_flux_lora_ui_kontext.py",
    "script_choices": [
                        "train_flux_lora_ui_kontext.py",
                        "train_flux_lora_ui_with_mask.py",
                        "train_flux_lora_ui.py",
                       ],
    "output_dir":"F:/models/flux",
    "save_name":"flux-lora",
    "pretrained_model_name_or_path":"F:/T2ITrainer/flux_models/kontext", # or local folder F:\Kolors
    "train_data_dir":"F:/ImageSet/kontext", 
    # "vae_path":None, # or local file
    "resume_from_checkpoint":None,
    "model_path":None, 
    # "logging_dir":"logs",
    "report_to":"wandb", 
    "rank":16,
    "train_batch_size":1,
    "repeats":1,
    "gradient_accumulation_steps":1,
    "mixed_precision":"bf16",
    "gradient_checkpointing":True,
    "optimizer":"adamw",
    "lr_scheduler":"constant", 
    "learning_rate":1e-4,
    "lr_warmup_steps":0,
    "seed":4321,
    "num_train_epochs":5,
    "save_model_epochs":1, 
    "validation_epochs":1, 
    "skip_epoch":0, 
    # "break_epoch":0,
    "skip_step":0, 
    "validation_ratio":0.1, 
    # "use_dora":False,
    "recreate_cache":False,
    "caption_dropout":0.1,
    "config_path":"config.json",
    "resolution":"512",
    "resolution_choices":["512","256","1024"],
    "use_debias":False,
    "snr_gamma":0,
    "cosine_restarts":1,
    "max_time_steps":0,
    "blocks_to_swap":0,
    "mask_dropout":0,
    "reg_ratio":0.7,
    "reg_timestep":700
    # "use_fp8":True
    # "freeze_transformer_layers":'5,7,10,17,18,19'
}


# Function to save configuration to a specified directory
def save_config( 
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
        # use_fp8
        # freeze_transformer_layers
    ):
    config = {
        "script":script,
        "seed":seed,
        # "logging_dir":logging_dir,
        "mixed_precision":mixed_precision,
        "report_to":report_to,
        "lr_warmup_steps":lr_warmup_steps,
        "output_dir":output_dir,
        "save_name":save_name,
        "train_data_dir":train_data_dir,
        "optimizer":optimizer,
        "lr_scheduler":lr_scheduler,
        "learning_rate":learning_rate,
        "train_batch_size":train_batch_size,
        "repeats":repeats,
        "gradient_accumulation_steps":gradient_accumulation_steps,
        "num_train_epochs":num_train_epochs,
        "save_model_epochs":save_model_epochs,
        "validation_epochs":validation_epochs,
        "rank":rank,
        "skip_epoch":skip_epoch,
        # "break_epoch":break_epoch,
        "skip_step":skip_step,
        "gradient_checkpointing":gradient_checkpointing,
        "validation_ratio":validation_ratio,
        "pretrained_model_name_or_path":pretrained_model_name_or_path,
        "model_path":model_path,
        "resume_from_checkpoint":resume_from_checkpoint,
        # "use_dora":use_dora,
        "recreate_cache":recreate_cache,
        # "vae_path":vae_path,
        "config_path":config_path,
        "resolution":resolution,
        # "use_debias":use_debias,
        # 'snr_gamma':snr_gamma,
        "caption_dropout":caption_dropout,
        "cosine_restarts":cosine_restarts,
        "max_time_steps":max_time_steps,
        # "freeze_transformer_layers":freeze_transformer_layers
        "blocks_to_swap":blocks_to_swap,
        "mask_dropout":mask_dropout,
        "reg_ratio":reg_ratio,
        "reg_timestep":reg_timestep
        # "use_fp8":use_fp8
    }
    # config_path = os.path.join(config_dir, f"{filename}{ext}")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    print(f"Configuration saved to {config_path}")
    print(f"Update default config")
    with open("config.json", 'w') as f:
        json.dump(config, f, indent=4)

# Function to load configuration from a specified directory
def load_config(config_path):
    if not config_path.endswith(".json"):
        print("!!!File is not json format.")
        print("Load default config")
        config_path = "config.json"
    if not os.path.exists(config_path):
        # create config
        with open(config_path, 'w') as f:
            config = {}
            for key in default_config.keys():
                config[key] = default_config[key]
            json.dump(config, f, indent=4)
        return config
    config_path = os.path.join(config_path)
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
    except:
        config_path = "config.json"
    print(f"Loaded configuration from {config_path}")
    # print("config")
    # print(config)
    for key in config.keys():
        default_config[key] = config[key]
            
    # print("default_config")
    # print(default_config)
    return config_path,default_config['script'],default_config['seed'], \
            default_config['mixed_precision'],default_config['report_to'],default_config['lr_warmup_steps'], \
            default_config['output_dir'],default_config['save_name'],default_config['train_data_dir'], \
            default_config['optimizer'],default_config['lr_scheduler'],default_config['learning_rate'], \
            default_config['train_batch_size'],default_config['repeats'],default_config['gradient_accumulation_steps'], \
            default_config['num_train_epochs'],default_config['save_model_epochs'],default_config['validation_epochs'], \
            default_config['rank'],default_config['skip_epoch'], \
            default_config['skip_step'],default_config['gradient_checkpointing'],default_config['validation_ratio'], \
            default_config['pretrained_model_name_or_path'],default_config['model_path'],default_config['resume_from_checkpoint'], \
            default_config['recreate_cache'],default_config['resolution'], \
            default_config['caption_dropout'], \
            default_config['cosine_restarts'],default_config['max_time_steps'], \
            default_config['blocks_to_swap'],default_config['mask_dropout'], \
            default_config['reg_ratio'],default_config['reg_timestep']
            # default_config['use_dora'], \
            # default_config['freeze_transformer_layers']
            # default_config['logging_dir'],default_config['break_epoch'], 

# load config.json by default
load_config("config.json")
def run(
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
        # use_fp8
        # freeze_transformer_layers
    ):
    # if vae_path is not None:
    #     if not vae_path.endswith('.safetensors') and not vae_path == "":
    #         msg = "Vae need to be a single file ends with .safetensors. It should be the fp16 fix vae from https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/tree/main"
    #         gr.Warning(msg)
    #         return msg
    
    inputs = {
        "seed":seed,
        # "logging_dir":logging_dir,
        "mixed_precision":mixed_precision,
        "report_to":report_to,
        "lr_warmup_steps":lr_warmup_steps,
        "output_dir":output_dir,
        "save_name":save_name,
        "train_data_dir":train_data_dir,
        "optimizer":optimizer,
        "lr_scheduler":lr_scheduler,
        "learning_rate":learning_rate,
        "train_batch_size":train_batch_size,
        "repeats":repeats,
        "gradient_accumulation_steps":gradient_accumulation_steps,
        "num_train_epochs":num_train_epochs,
        "save_model_epochs":save_model_epochs,
        "validation_epochs":validation_epochs,
        "rank":rank,
        "skip_epoch":skip_epoch,
        # "break_epoch":break_epoch,
        "skip_step":skip_step,
        "gradient_checkpointing":gradient_checkpointing,
        "validation_ratio":validation_ratio,
        "pretrained_model_name_or_path":pretrained_model_name_or_path,
        "model_path":model_path,
        "resume_from_checkpoint":resume_from_checkpoint,
        # "use_dora":use_dora,
        "recreate_cache":recreate_cache,
        # "vae_path":vae_path,
        "resolution":resolution,
        # "use_debias":use_debias,
        # "snr_gamma":snr_gamma,
        "caption_dropout":caption_dropout,
        "cosine_restarts":cosine_restarts,
        "max_time_steps":max_time_steps,
        "blocks_to_swap":blocks_to_swap,
        "mask_dropout":mask_dropout,
        "reg_ratio":reg_ratio,
        "reg_timestep":reg_timestep
        # "use_fp8":use_fp8
        # "freeze_transformer_layers":freeze_transformer_layers
    }
    # Convert the inputs dictionary to a list of arguments
    # args = ["python", "train_sd3_lora_ui.py"]  # replace "your_script.py" with the name of your script
    # script = "test_.pyt"
    args = [sys.executable, script]
    for key, value in inputs.items():
        if value is not None:
            if isinstance(value, bool):  # exclude boolean values
                if value == True:
                    args.append(f"--{key}")
            else:
                args.append(f"--{key}")
                args.append(str(value))
    
    # Add the config_path argument if the script is train_flux_lora_ui_with_mask_cat_custom3.py
    # if script == "train_flux_lora_ui_with_mask_cat_custom3.py":
    #     args.append("--config_path")
    #     args.append(config_path)
    
    # Call the script with the arguments
    subprocess.call(args)
    save_config(
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
        # use_fp8
        # freeze_transformer_layers
    )
    # print(args)
    return " ".join(args)
    

with gr.Blocks(title="Flux Kontext LoRA è®­ç»ƒç•Œé¢", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
    """
    # ğŸ¨ Flux Kontext LoRA è®­ç»ƒç•Œé¢
    ä¸“ä¸ºFlux Kontextæ¨¡å‹LoRAè®­ç»ƒè®¾è®¡çš„å›¾å½¢ç•Œé¢
    """)
    script = gr.Dropdown(label="ğŸ”§ è®­ç»ƒè„šæœ¬", value=default_config["script"], choices=default_config["script_choices"], info="é€‰æ‹©è¦ä½¿ç”¨çš„è®­ç»ƒè„šæœ¬")
    with gr.Row(equal_height=True):
            # Text input for user to specify another config save and load dir
        config_path = gr.Textbox(scale=3, label="ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„ (.jsonæ–‡ä»¶)", value=default_config["config_path"], placeholder="è¾“å…¥ä¿å­˜/åŠ è½½é…ç½®çš„è·¯å¾„")
        save_config_btn = gr.Button("ğŸ’¾ ä¿å­˜é…ç½®", scale=1, variant="secondary")
        load_config_btn = gr.Button("ğŸ“‚ åŠ è½½é…ç½®", scale=1, variant="secondary")

    with gr.Accordion("ğŸ“‚ ç›®å½•é…ç½®", open=True):
        # dir section
        with gr.Row():
            output_dir = gr.Textbox(label="ğŸ“¤ è¾“å‡ºç›®å½•", value=default_config["output_dir"],
                                   placeholder="æ¨¡å‹ä¿å­˜ä½ç½®")
            save_name = gr.Textbox(label="ğŸ’¾ ä¿å­˜åç§°", value=default_config["save_name"],
                                   placeholder="æ¨¡å‹ä¿å­˜åç§°")
        with gr.Row():
            pretrained_model_name_or_path = gr.Textbox(label="ğŸ¤– é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„", 
                value=default_config["pretrained_model_name_or_path"], 
                placeholder="æ¨¡å‹ä»“åº“åç§°æˆ–åŒ…å«diffusersæ¨¡å‹ç»“æ„çš„ç›®å½•"
            )
            resume_from_checkpoint = gr.Textbox(label="ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤", value=default_config["resume_from_checkpoint"], placeholder="ä»é€‰å®šç›®å½•æ¢å¤LoRAæƒé‡")
        with gr.Row():
            train_data_dir = gr.Textbox(label="ğŸ“Š è®­ç»ƒæ•°æ®ç›®å½•", value=default_config["train_data_dir"], placeholder="åŒ…å«æ•°æ®é›†çš„ç›®å½•")
            model_path = gr.Textbox(label="ğŸ“„ æ¨¡å‹æ–‡ä»¶è·¯å¾„", value=default_config["model_path"], placeholder="å•ä¸ªæƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸æ˜¯ä»å®˜æ–¹æƒé‡è®­ç»ƒï¼‰")
            # logging_dir = gr.Textbox(label="logging_dir", value=default_config["logging_dir"], placeholder="logs folder")
        with gr.Row():
            report_to = gr.Dropdown(label="ğŸ“ˆ æŠ¥å‘Šå¹³å°", value=default_config["report_to"], choices=["wandb"], info="è®­ç»ƒæŒ‡æ ‡æŠ¥å‘Šå¹³å°")
            

    with gr.Accordion("âš™ï¸ LoRA é…ç½®", open=True):
        # train related section
        with gr.Row():
            rank = gr.Number(label="ğŸ”¢ LoRA Rank", value=default_config["rank"], info="å»ºè®®ï¼šè®­ç»ƒé›†å°‘äº100æ—¶ä½¿ç”¨rank 4ï¼Œä¸€èˆ¬ç”¨16-32" )
            train_batch_size = gr.Number(label="ğŸ“¦ è®­ç»ƒæ‰¹æ¬¡å¤§å°", value=default_config["train_batch_size"], info="æ‰¹æ¬¡å¤§å°1ä½¿ç”¨18GBæ˜¾å­˜ï¼Œè¯·ä½¿ç”¨å°æ‰¹æ¬¡é¿å…æ˜¾å­˜ä¸è¶³" )
        with gr.Row():
            repeats = gr.Number(label="ğŸ”„ é‡å¤æ¬¡æ•°", value=default_config["repeats"], info="æ¯ä¸ªæ ·æœ¬é‡å¤è®­ç»ƒæ¬¡æ•°")
            gradient_accumulation_steps = gr.Number(label="ğŸ“ˆ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°", value=default_config["gradient_accumulation_steps"], info="ç´¯ç§¯æ¢¯åº¦å‡å°‘æ˜¾å­˜ä½¿ç”¨")
            # mixed_precision = gr.Radio(label="mixed_precision", value=default_config["mixed_precision"], choices=["fp16", "bf16"])
            mixed_precision = gr.Radio(label="ğŸ’¾ æ··åˆç²¾åº¦", value=default_config["mixed_precision"], choices=["bf16", "fp8"], info="æ¨èbf16")
            gradient_checkpointing = gr.Checkbox(label="ğŸ” æ¢¯åº¦æ£€æŸ¥ç‚¹", value=default_config["gradient_checkpointing"], info="èŠ‚çœæ˜¾å­˜ä½†ç¨æ…¢")
            # use_dora = gr.Checkbox(label="use_dora", value=default_config["use_dora"])
        with gr.Row():
            optimizer = gr.Dropdown(label="ğŸ¯ ä¼˜åŒ–å™¨", value=default_config["optimizer"], choices=["adamw","prodigy"], info="æ¨èAdamW")
            lr_scheduler = gr.Dropdown(label="ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨", value=default_config["lr_scheduler"], 
                        choices=["linear", "cosine", "cosine_with_restarts", "polynomial","constant", "constant_with_warmup"], info="æ¨ècosine")
            cosine_restarts = gr.Number(label="ğŸ”„ ä½™å¼¦é‡å¯æ¬¡æ•°", value=default_config["cosine_restarts"], info="ä»…å¯¹cosine_with_restartsæœ‰æ•ˆ", minimum=1)
        with gr.Row():
            learning_rate = gr.Number(label="ğŸ“ˆ å­¦ä¹ ç‡", value=default_config["learning_rate"], info="æ¨èï¼š1e-4ï¼ˆAdamWï¼‰æˆ–1ï¼ˆProdigyï¼‰")
            lr_warmup_steps = gr.Number(label="ğŸ”¥ å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°", value=default_config["lr_warmup_steps"], info="é¢„çƒ­æ­¥æ•°")
            seed = gr.Number(label="ğŸ² éšæœºç§å­", value=default_config["seed"], info="å›ºå®šéšæœºç§å­ä¿è¯å¯é‡ç°æ€§")
        with gr.Row():
            blocks_to_swap = gr.Number(label="ğŸ’¾ å—äº¤æ¢æ•°é‡", value=default_config["blocks_to_swap"], info="äº¤æ¢åˆ°CPUçš„å—æ•°é‡ã€‚24GBæ˜¾å­˜å»ºè®®10ï¼Œæ›´ä½æ˜¾å­˜ç”¨æ›´å¤š" )
            mask_dropout = gr.Number(label="ğŸ­ æ©ç ä¸¢å¼ƒç‡", value=default_config["mask_dropout"], info="æ©ç ä¸¢å¼ƒæ„å‘³ç€æ©ç å…¨ä¸º1è¿›è¡Œæ•´å›¾é‡å»º" )
        #     freeze_transformer_layers = gr.Textbox(label="freeze_transformer_layers", value=default_config["freeze_transformer_layers"], info="Stop training the transformer layers included in the input using ',' to seperate layers. Example: 5,7,10,17,18,19" )
            reg_ratio = gr.Number(label="ğŸ“Š æ­£åˆ™åŒ–æ¯”ä¾‹", value=default_config["reg_ratio"], info="ç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡è®¾ä¸º1" )
            reg_timestep = gr.Number(label="â° æ­£åˆ™åŒ–æ—¶é—´æ­¥", value=default_config["reg_timestep"], info="ç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡è®¾ä¸º0" )
            
            
    with gr.Accordion("ğŸ”§ å…¶ä»–è®¾ç½®"):
        with gr.Row():
            num_train_epochs = gr.Number(label="ğŸ”„ è®­ç»ƒè½®æ•°", value=default_config["num_train_epochs"], info="è®­ç»ƒçš„æ€»è½®æ•°")
            save_model_epochs = gr.Number(label="ğŸ’¾ ä¿å­˜æ¨¡å‹é—´éš”", value=default_config["save_model_epochs"], info="æ¯xè½®ä¿å­˜æ£€æŸ¥ç‚¹")
            validation_epochs = gr.Number(label="ğŸ” éªŒè¯é—´éš”", value=default_config["validation_epochs"], info="æ¯xè½®æ‰§è¡ŒéªŒè¯")
        with gr.Row():
            skip_epoch = gr.Number(label="â­ï¸ è·³è¿‡è½®æ•°", value=default_config["skip_epoch"], info="è·³è¿‡xè½®è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹")
            # break_epoch = gr.Number(label="break_epoch", value=default_config["break_epoch"], info="Stop train after x epoches")
            skip_step = gr.Number(label="â­ï¸ è·³è¿‡æ­¥æ•°", value=default_config["skip_step"], info="è·³è¿‡xæ­¥è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹")
            validation_ratio = gr.Number(label="ğŸ“Š éªŒè¯é›†æ¯”ä¾‹", value=default_config["validation_ratio"], info="æŒ‰æ­¤æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†ç”¨äºéªŒè¯")
            
        with gr.Row():
            recreate_cache = gr.Checkbox(label="ğŸ”„ é‡æ–°åˆ›å»ºç¼“å­˜", value=default_config["recreate_cache"], info="å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰ç¼“å­˜æ–‡ä»¶")
            # use_debias = gr.Checkbox(label="use_debias", value=default_config["use_debias"])
            # snr_gamma = gr.Number(label="min-snr_gamma recommanded: 5", value=default_config["snr_gamma"], info="Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.", maximum=10, minimum=0)
            caption_dropout = gr.Number(label="ğŸ’¬ æ ‡é¢˜ä¸¢å¼ƒç‡", value=default_config["caption_dropout"], info="éšæœºä¸¢å¼ƒæ ‡é¢˜çš„æ¦‚ç‡", maximum=1, minimum=0)
            max_time_steps = gr.Number(label="â° æœ€å¤§æ—¶é—´æ­¥é™åˆ¶", value=default_config["max_time_steps"], info="æœ€å¤§æ—¶é—´æ­¥æ•°é™åˆ¶", maximum=1000, minimum=0)
        gr.Markdown(
"""
## ğŸ¯ å®éªŒé€‰é¡¹ï¼šåˆ†è¾¨ç‡
- åŸºäºç›®æ ‡åˆ†è¾¨ç‡ï¼ˆé»˜è®¤ï¼š1024ï¼‰
- æ”¯æŒ512æˆ–1024åˆ†è¾¨ç‡
- **æ¨è**ï¼šæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨512ï¼Œå……è¶³æ—¶ä½¿ç”¨1024
""")
        with gr.Row():
            resolution = gr.Dropdown(label="ğŸ–¼ï¸ è®­ç»ƒåˆ†è¾¨ç‡", value=default_config["resolution"], choices=default_config["resolution_choices"], info="æ›´é«˜åˆ†è¾¨ç‡éœ€è¦æ›´å¤šæ˜¾å­˜")
    inputs = [
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
        # freeze_transformer_layers,
    ]
    # æ·»åŠ è®­ç»ƒæç¤ºä¿¡æ¯
    gr.Markdown(
    """
    ---
    ### ğŸ’¡ è®­ç»ƒæç¤º
    - **æ˜¾å­˜ä¸è¶³**ï¼šå‡å°æ‰¹æ¬¡å¤§å°ã€å¢åŠ å—äº¤æ¢æ•°é‡ã€é™ä½åˆ†è¾¨ç‡
    - **æ¨èè®¾ç½®**ï¼šRank 16-32ï¼Œå­¦ä¹ ç‡1e-4ï¼ŒAdamWä¼˜åŒ–å™¨ï¼Œbf16ç²¾åº¦
    - **æ•°æ®å‡†å¤‡**ï¼šç¡®ä¿è®­ç»ƒæ•°æ®ç›®å½•åŒ…å«é…å¯¹çš„_R(å‚è€ƒ)å’Œ_T(ç›®æ ‡)å›¾åƒåŠå¯¹åº”çš„.txtæ–‡ä»¶
    """
    )
    
    output = gr.Textbox(label="ğŸ“„ è®­ç»ƒè¾“å‡ºæ—¥å¿—", lines=10, max_lines=20, show_copy_button=True, interactive=False)
    
    with gr.Row():
        run_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
        
    # çŠ¶æ€æ˜¾ç¤º
    gr.Markdown(
    """
    ---
    ### ğŸ“Š å½“å‰é…ç½®çŠ¶æ€
    è¯·ç¡®è®¤ä»¥ä¸Šè®¾ç½®åç‚¹å‡»"å¼€å§‹è®­ç»ƒ"æŒ‰é’®ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯¦ç»†æ—¥å¿—å°†åœ¨ä¸Šæ–¹è¾“å‡ºæ¡†ä¸­æ˜¾ç¤ºã€‚
    """
    )
    # inputs.append(config_path)
    run_btn.click(fn=run, inputs=inputs, outputs=output, api_name="run")
    save_config_btn.click(fn=save_config, inputs=inputs)
    load_config_btn.click(fn=load_config, inputs=[config_path], outputs=inputs)
demo.launch()