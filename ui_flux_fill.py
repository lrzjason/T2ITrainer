import gradio as gr

import subprocess
import json
import sys
import os

# ===== è¯­è¨€ç¿»è¯‘ç³»ç»Ÿ =====
# ç¿»è¯‘å­—å…¸ - åŒ…å«æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
TRANSLATIONS = {
    'zh': {
        'title': '## LoRA è®­ç»ƒ',
        'script': 'è®­ç»ƒè„šæœ¬',
        'config_path': 'é…ç½®æ–‡ä»¶è·¯å¾„ (.jsonæ–‡ä»¶)',
        'config_path_placeholder': 'è¾“å…¥ä¿å­˜/åŠ è½½é…ç½®çš„è·¯å¾„',
        'save': 'ä¿å­˜',
        'load': 'åŠ è½½',
        'directory_section': 'ç›®å½•é…ç½®',
        'output_dir': 'è¾“å‡ºç›®å½•',
        'output_dir_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®',
        'save_name': 'ä¿å­˜åç§°',
        'save_name_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜åç§°',
        'pretrained_model_name_or_path': 'é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„',
        'pretrained_model_placeholder': 'ä»“åº“åç§°æˆ–åŒ…å«diffusersæ¨¡å‹ç»“æ„çš„ç›®å½•',
        'resume_from_checkpoint': 'ä»æ£€æŸ¥ç‚¹æ¢å¤',
        'resume_checkpoint_placeholder': 'ä»é€‰å®šç›®å½•æ¢å¤loraæƒé‡',
        'train_data_dir': 'è®­ç»ƒæ•°æ®ç›®å½•',
        'train_data_dir_placeholder': 'åŒ…å«æ•°æ®é›†çš„ç›®å½•',
        'model_path': 'æ¨¡å‹è·¯å¾„',
        'model_path_placeholder': 'å¦‚æœä¸æ˜¯ä»å®˜æ–¹æƒé‡è®­ç»ƒåˆ™ä¸ºå•ä¸ªæƒé‡æ–‡ä»¶',
        'report_to': 'æŠ¥å‘Šåˆ°',
        'lora_config': 'LoRA é…ç½®',
        'rank': 'ç§©',
        'rank_info': 'å»ºè®®å¯¹å°äº100çš„è®­ç»ƒé›†ä½¿ç”¨ç§©4',
        'algo': 'ç®—æ³•',
        'conv_dim': 'å·ç§¯ç»´åº¦',
        'conv_alpha': 'å·ç§¯Alpha',
        'train_batch_size': 'è®­ç»ƒæ‰¹æ¬¡å¤§å°',
        'batch_size_info': 'æ‰¹æ¬¡å¤§å°1ä½¿ç”¨18GBã€‚è¯·ä½¿ç”¨å°æ‰¹æ¬¡å¤§å°ä»¥é¿å…å†…å­˜ä¸è¶³',
        'repeats': 'é‡å¤æ¬¡æ•°',
        'gradient_accumulation_steps': 'æ¢¯åº¦ç´¯ç§¯æ­¥æ•°',
        'mixed_precision': 'æ··åˆç²¾åº¦',
        'gradient_checkpointing': 'æ¢¯åº¦æ£€æŸ¥ç‚¹',
        'optimizer': 'ä¼˜åŒ–å™¨',
        'lr_scheduler': 'å­¦ä¹ ç‡è°ƒåº¦å™¨',
        'cosine_restarts': 'ä½™å¼¦é‡å¯',
        'cosine_restarts_info': 'ä»…å¯¹å­¦ä¹ ç‡è°ƒåº¦å™¨cosine_with_restartsæœ‰ç”¨',
        'learning_rate': 'å­¦ä¹ ç‡',
        'learning_rate_info': 'æ¨èï¼š1e-4 æˆ– prodigyä½¿ç”¨1',
        'lr_warmup_steps': 'å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°',
        'seed': 'éšæœºç§å­',
        'blocks_to_swap': 'äº¤æ¢å—æ•°',
        'blocks_to_swap_info': 'äº¤æ¢åˆ°CPUçš„å—æ•°ã€‚å»ºè®®24GBä½¿ç”¨10ï¼Œæ›´ä½æ˜¾å­˜ä½¿ç”¨æ›´å¤š',
        'mask_dropout': 'æ©ç ä¸¢å¼ƒ',
        'mask_dropout_info': 'ä¸¢å¼ƒæ©ç ï¼Œæ„å‘³ç€æ•´ä¸ªå›¾åƒé‡å»ºçš„æ©ç å…¨ä¸º1',
        'reg_ratio': 'æ­£åˆ™åŒ–æ¯”ç‡',
        'reg_ratio_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º1',
        'reg_timestep': 'æ­£åˆ™åŒ–æ—¶é—´æ­¥',
        'reg_timestep_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º0',
        'misc': 'æ‚é¡¹',
        'num_train_epochs': 'è®­ç»ƒè½®æ•°',
        'num_train_epochs_info': 'è®­ç»ƒçš„æ€»è½®æ•°',
        'save_model_epochs': 'ä¿å­˜æ¨¡å‹è½®æ•°',
        'save_model_epochs_info': 'æ¯xè½®ä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_epochs': 'éªŒè¯è½®æ•°',
        'validation_epochs_info': 'æ¯xè½®æ‰§è¡ŒéªŒè¯',
        'skip_epoch': 'è·³è¿‡è½®æ•°',
        'skip_epoch_info': 'è·³è¿‡xè½®è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'skip_step': 'è·³è¿‡æ­¥æ•°',
        'skip_step_info': 'è·³è¿‡xæ­¥è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_ratio': 'éªŒè¯æ¯”ä¾‹',
        'validation_ratio_info': 'æŒ‰æ­¤æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†ç”¨äºéªŒè¯',
        'recreate_cache': 'é‡æ–°åˆ›å»ºç¼“å­˜',
        'caption_dropout': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'caption_dropout_info': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'max_time_steps': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        'max_time_steps_info': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        'resolution_section': '## å®éªŒé€‰é¡¹ï¼šåˆ†è¾¨ç‡\n- åŸºäºç›®æ ‡åˆ†è¾¨ç‡ï¼ˆé»˜è®¤ï¼š1024ï¼‰ã€‚\n- æ”¯æŒ512æˆ–1024ã€‚',
        'resolution': 'åˆ†è¾¨ç‡',
        'output_box': 'è¾“å‡ºæ¡†',
        'run': 'è¿è¡Œ',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°English',
        
        'slider': 'æ»‘å—è®­ç»ƒç›¸å…³',
        'use_two_captions': 'ä½¿ç”¨ä¸¤å¯¹åº”æ–‡æœ¬æ ‡æ³¨',
        'slider_positive_scale': 'æ»‘å—æ­£å‘ç›®æ ‡å¼ºåº¦',
        'slider_negative_scale': 'æ»‘å—è´Ÿé¢ç›®æ ‡å¼ºåº¦'
    },
    'en': {
        'title': '## Lora Training',
        'script': 'script',
        'config_path': 'Config Path (.json file)',
        'config_path_placeholder': 'Enter path to save/load config',
        'save': 'Save',
        'load': 'load',
        'directory_section': 'Directory section',
        'output_dir': 'output_dir',
        'output_dir_placeholder': 'checkpoint save to',
        'save_name': 'save_name',
        'save_name_placeholder': 'checkpoint save name',
        'pretrained_model_name_or_path': 'pretrained_model_name_or_path',
        'pretrained_model_placeholder': 'repo name or dir contains diffusers model structure',
        'resume_from_checkpoint': 'resume_from_checkpoint',
        'resume_checkpoint_placeholder': 'resume the lora weight from seleted dir',
        'train_data_dir': 'train_data_dir',
        'train_data_dir_placeholder': 'dir contains dataset',
        'model_path': 'model_path',
        'model_path_placeholder': 'single weight files if not trained from official weight',
        'report_to': 'report_to',
        'lora_config': 'Lora Config',
        'rank': 'rank',
        'rank_info': 'Recommanded to use rank 4 for training set small than 100.',
        'algo': 'algo',
        'conv_dim': 'conv dim',
        'conv_alpha': 'conv alpha',
        'train_batch_size': 'train_batch_size',
        'batch_size_info': 'Batch size 1 is using 18GB. Please use small batch size to avoid oom.',
        'repeats': 'repeats',
        'gradient_accumulation_steps': 'gradient_accumulation_steps',
        'mixed_precision': 'mixed_precision',
        'gradient_checkpointing': 'gradient_checkpointing',
        'optimizer': 'optimizer',
        'lr_scheduler': 'lr_scheduler',
        'cosine_restarts': 'cosine_restarts',
        'cosine_restarts_info': 'Only useful for lr_scheduler: cosine_with_restarts',
        'learning_rate': 'learning_rate',
        'learning_rate_info': 'Recommended: 1e-4 or 1 for prodigy',
        'lr_warmup_steps': 'lr_warmup_steps',
        'seed': 'seed',
        'blocks_to_swap': 'blocks_to_swap',
        'blocks_to_swap_info': 'How many blocks to swap to cpu. It is suggested 10 for 24 GB and more for lower VRAM',
        'mask_dropout': 'mask_dropout',
        'mask_dropout_info': 'Dropout mask which means mask is all one for whole image reconstruction',
        'reg_ratio': 'reg_ratio',
        'reg_ratio_info': 'As regularization of objective transfer learning. Set as 1 if you aren\'t training different objective.',
        'reg_timestep': 'reg_timestep',
        'reg_timestep_info': 'As regularization of objective transfer learning. Set as 0 if you aren\'t training different objective.',
        'misc': 'Misc',
        'num_train_epochs': 'num_train_epochs',
        'num_train_epochs_info': 'Total epoches of the training',
        'save_model_epochs': 'save_model_epochs',
        'save_model_epochs_info': 'Save checkpoint when x epoches',
        'validation_epochs': 'validation_epochs',
        'validation_epochs_info': 'perform validation when x epoches',
        'skip_epoch': 'skip_epoch',
        'skip_epoch_info': 'Skip x epoches for validation and save checkpoint',
        'skip_step': 'skip_step',
        'skip_step_info': 'Skip x steps for validation and save checkpoint',
        'validation_ratio': 'validation_ratio',
        'validation_ratio_info': 'Split dataset with this ratio for validation',
        'recreate_cache': 'recreate_cache',
        'caption_dropout': 'Caption Dropout',
        'caption_dropout_info': 'Caption Dropout',
        'max_time_steps': 'Max timesteps limitation',
        'max_time_steps_info': 'Max timesteps limitation',
        'resolution_section': '## Experiment Option: resolution\n- Based target resolution (default:1024). \n- 512 or 1024 are supported.',
        'resolution': 'resolution',
        'output_box': 'Output Box',
        'run': 'Run',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°ä¸­æ–‡',
        
        
        'slider': 'Slider Related',
        'use_two_captions': 'Use two captions for each direction',
        'slider_positive_scale': 'Slider positive scale',
        'slider_negative_scale': 'Slider negative scale'
    }
}

# å½“å‰è¯­è¨€çŠ¶æ€
current_language = 'en'  # é»˜è®¤ä¸­æ–‡

def get_text(key):
    """è·å–å½“å‰è¯­è¨€çš„æ–‡æœ¬"""
    return TRANSLATIONS[current_language].get(key, key)

def toggle_language():
    """åˆ‡æ¢è¯­è¨€"""
    global current_language
    current_language = 'en' if current_language == 'zh' else 'zh'
    return current_language

default_config = {
    "script": "train_flux_lora_ui_kontext.py",
    "script_choices": [
                        "train_flux_lora_ui_kontext.py",
                        "train_flux_lora_ui_kontext_slider.py",
                        "train_flux_lora_ui_with_mask.py",
                        "train_flux_lora_ui.py",
                       ],
    "output_dir":"F:/models/flux",
    "save_name":"flux-lora",
    "pretrained_model_name_or_path":"F:/T2ITrainer/flux_models/kontext", # or local folder F:\Kolors
    "train_data_dir":"F:/ImageSet/kontext", 
    # "vae_path":None, # or local file
    "resume_from_checkpoint":None,
    "model_path":None, 
    # "logging_dir":"logs",
    "report_to":"all",
    "rank":16,
    "algo":"locon",
    "conv_dim":16,
    "conv_alpha":0.5,
    "train_batch_size":1,
    "repeats":1,
    "gradient_accumulation_steps":1,
    "mixed_precision":"bf16",
    "gradient_checkpointing":True,
    "optimizer":"adamw",
    "lr_scheduler":"constant", 
    "learning_rate":1e-4,
    "lr_warmup_steps":0,
    "seed":4321,
    "num_train_epochs":5,
    "save_model_epochs":1, 
    "validation_epochs":1, 
    "skip_epoch":0, 
    # "break_epoch":0,
    "skip_step":0, 
    "validation_ratio":0.1, 
    # "use_dora":False,
    "recreate_cache":False,
    "caption_dropout":0.1,
    "config_path":"config.json",
    "resolution":"512",
    "resolution_choices":["512","256","1024"],
    "use_debias":False,
    "snr_gamma":0,
    "cosine_restarts":1,
    "max_time_steps":0,
    "blocks_to_swap":0,
    "mask_dropout":0,
    "reg_ratio":0.0,
    "reg_timestep":0,
    'use_two_captions': False,
    'slider_positive_scale': 1.0,
    'slider_negative_scale': -1.0
    # "use_fp8":True
    # "freeze_transformer_layers":'5,7,10,17,18,19'
}


# Function to save configuration to a specified directory
def save_config( 
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        algo,
        conv_dim,
        conv_alpha,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep,
        use_two_captions,
        slider_positive_scale,
        slider_negative_scale
        # use_fp8
        # freeze_transformer_layers
    ):
    config = {
        "script":script,
        "seed":seed,
        # "logging_dir":logging_dir,
        "mixed_precision":mixed_precision,
        "report_to":report_to,
        "lr_warmup_steps":lr_warmup_steps,
        "output_dir":output_dir,
        "save_name":save_name,
        "train_data_dir":train_data_dir,
        "optimizer":optimizer,
        "lr_scheduler":lr_scheduler,
        "learning_rate":learning_rate,
        "train_batch_size":train_batch_size,
        "repeats":repeats,
        "gradient_accumulation_steps":gradient_accumulation_steps,
        "num_train_epochs":num_train_epochs,
        "save_model_epochs":save_model_epochs,
        "validation_epochs":validation_epochs,
        "rank":rank,
        "algo":algo,
        "conv_dim":conv_dim,
        "conv_alpha":conv_alpha,
        "skip_epoch":skip_epoch,
        # "break_epoch":break_epoch,
        "skip_step":skip_step,
        "gradient_checkpointing":gradient_checkpointing,
        "validation_ratio":validation_ratio,
        "pretrained_model_name_or_path":pretrained_model_name_or_path,
        "model_path":model_path,
        "resume_from_checkpoint":resume_from_checkpoint,
        # "use_dora":use_dora,
        "recreate_cache":recreate_cache,
        # "vae_path":vae_path,
        "config_path":config_path,
        "resolution":resolution,
        # "use_debias":use_debias,
        # 'snr_gamma':snr_gamma,
        "caption_dropout":caption_dropout,
        "cosine_restarts":cosine_restarts,
        "max_time_steps":max_time_steps,
        # "freeze_transformer_layers":freeze_transformer_layers
        "blocks_to_swap":blocks_to_swap,
        "mask_dropout":mask_dropout,
        "reg_ratio":reg_ratio,
        "reg_timestep":reg_timestep,
        'use_two_captions': use_two_captions,
        'slider_positive_scale': slider_positive_scale,
        'slider_negative_scale': slider_negative_scale
        # "use_fp8":use_fp8
    }
    # config_path = os.path.join(config_dir, f"{filename}{ext}")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    print(f"Configuration saved to {config_path}")
    print(f"Update default config")
    with open("config.json", 'w') as f:
        json.dump(config, f, indent=4)

# Function to load configuration from a specified directory
def load_config(config_path):
    if not config_path.endswith(".json"):
        print("!!!File is not json format.")
        print("Load default config")
        config_path = "config.json"
    if not os.path.exists(config_path):
        # create config
        with open(config_path, 'w') as f:
            config = {}
            for key in default_config.keys():
                config[key] = default_config[key]
            json.dump(config, f, indent=4)
        return config
    config_path = os.path.join(config_path)
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
    except:
        config_path = "config.json"
    print(f"Loaded configuration from {config_path}")
    # print("config")
    # print(config)
    for key in config.keys():
        default_config[key] = config[key]
            
    # print("default_config")
    # print(default_config)
    return config_path,default_config['script'],default_config['seed'], \
            default_config['mixed_precision'],default_config['report_to'],default_config['lr_warmup_steps'], \
            default_config['output_dir'],default_config['save_name'],default_config['train_data_dir'], \
            default_config['optimizer'],default_config['lr_scheduler'],default_config['learning_rate'], \
            default_config['train_batch_size'],default_config['repeats'],default_config['gradient_accumulation_steps'], \
            default_config['num_train_epochs'],default_config['save_model_epochs'],default_config['validation_epochs'], \
            default_config['rank'],default_config['algo'],default_config['conv_dim'],default_config['conv_alpha'],default_config['skip_epoch'], \
            default_config['skip_step'],default_config['gradient_checkpointing'],default_config['validation_ratio'], \
            default_config['pretrained_model_name_or_path'],default_config['model_path'],default_config['resume_from_checkpoint'], \
            default_config['recreate_cache'],default_config['resolution'], \
            default_config['caption_dropout'], \
            default_config['cosine_restarts'],default_config['max_time_steps'], \
            default_config['blocks_to_swap'],default_config['mask_dropout'], \
            default_config['reg_ratio'],default_config['reg_timestep'], \
            default_config['use_two_captions'],default_config['slider_positive_scale'],default_config['slider_negative_scale']
            # default_config['use_dora'], \
            # default_config['freeze_transformer_layers']
            # default_config['logging_dir'],default_config['break_epoch'], 

# load config.json by default
load_config("config.json")
def run(
        config_path,
        script,
        seed,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        algo,
        conv_dim,
        conv_alpha,
        skip_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        recreate_cache,
        resolution,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep,
        use_two_captions,
        slider_positive_scale,
        slider_negative_scale
    ):
    # Save the current configuration to the specified config file
    save_config(
        config_path,
        script,
        seed,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        algo,
        conv_dim,
        conv_alpha,
        skip_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        recreate_cache,
        resolution,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep,
        use_two_captions,
        slider_positive_scale,
        slider_negative_scale
    )

    # Construct the command to run the script with only the config path
    command_args = [sys.executable, script, "--config_path", config_path]

    # Execute the command
    subprocess.call(command_args)

    # Return the executed command as a string
    return " ".join(command_args)


    

def toggle_language_handler():
    """è¯­è¨€åˆ‡æ¢å¤„ç†å‡½æ•°"""
    toggle_language()
    # è¿”å›æ›´æ–°åçš„ç»„ä»¶
    updates = []
    # æ›´æ–°æ ‡é¢˜
    updates.append(gr.Markdown(get_text('title')))
    # æ›´æ–°è¯­è¨€åˆ‡æ¢æŒ‰é’®æ–‡æœ¬
    updates.append(gr.Button(get_text('language_toggle'), scale=0, size="sm"))
    return updates

# åˆ›å»ºUIç•Œé¢
with gr.Blocks() as demo:
    # è¯­è¨€åˆ‡æ¢æŒ‰é’®
    with gr.Row():
        gr.HTML("<div style='flex-grow: 1;'></div>")  # å ä½ç¬¦ï¼Œè®©æŒ‰é’®å³å¯¹é½
        language_toggle_btn = gr.Button(get_text('language_toggle'), scale=0, size="sm")
    
    # æ ‡é¢˜
    title_md = gr.Markdown(get_text('title'))
    
    # è„šæœ¬é€‰æ‹©
    script = gr.Dropdown(label=get_text('script'), value=default_config["script"], choices=default_config["script_choices"])
    
    # é…ç½®æ–‡ä»¶æ“ä½œ
    with gr.Row(equal_height=True):
        config_path = gr.Textbox(scale=3, label=get_text('config_path'), value=default_config["config_path"], placeholder=get_text('config_path_placeholder'))
        save_config_btn = gr.Button(get_text('save'), scale=1)
        load_config_btn = gr.Button(get_text('load'), scale=1)

    directory_accordion = gr.Accordion(get_text('directory_section'))
    with directory_accordion:
        # ç›®å½•è®¾ç½®éƒ¨åˆ†
        with gr.Row():
            output_dir = gr.Textbox(label=get_text('output_dir'), value=default_config["output_dir"],
                                   placeholder=get_text('output_dir_placeholder'))
            save_name = gr.Textbox(label=get_text('save_name'), value=default_config["save_name"],
                                   placeholder=get_text('save_name_placeholder'))
        with gr.Row():
            pretrained_model_name_or_path = gr.Textbox(label=get_text('pretrained_model_name_or_path'), 
                value=default_config["pretrained_model_name_or_path"], 
                placeholder=get_text('pretrained_model_name_or_path_placeholder')
            )
            resume_from_checkpoint = gr.Textbox(label=get_text('resume_from_checkpoint'), value=default_config["resume_from_checkpoint"], placeholder=get_text('resume_from_checkpoint_placeholder'))
        with gr.Row():
            train_data_dir = gr.Textbox(label=get_text('train_data_dir'), value=default_config["train_data_dir"], placeholder=get_text('train_data_dir_placeholder'))
            model_path = gr.Textbox(label=get_text('model_path'), value=default_config["model_path"], placeholder=get_text('model_path_placeholder'))
        with gr.Row():
            report_to = gr.Dropdown(label=get_text('report_to'), value=default_config["report_to"], choices=["all","wandb","tensorboard"])

    lora_accordion = gr.Accordion(get_text('lora_config'))
    with lora_accordion:
        # è®­ç»ƒç›¸å…³è®¾ç½®
        with gr.Row():
            rank = gr.Number(label=get_text('rank'), value=default_config["rank"], info=get_text('rank_info'))
            algo = gr.Textbox(label=get_text('algo'), value=default_config["algo"])
            conv_dim = gr.Number(label=get_text('conv_dim'), value=default_config["conv_dim"])
            conv_alpha = gr.Number(label=get_text('conv_alpha'), value=default_config["conv_alpha"])
            train_batch_size = gr.Number(label=get_text('train_batch_size'), value=default_config["train_batch_size"], info=get_text('train_batch_size_info'))
        with gr.Row():
            repeats = gr.Number(label=get_text('repeats'), value=default_config["repeats"])
            gradient_accumulation_steps = gr.Number(label=get_text('gradient_accumulation_steps'), value=default_config["gradient_accumulation_steps"])
            mixed_precision = gr.Radio(label=get_text('mixed_precision'), value=default_config["mixed_precision"], choices=["bf16", "fp8"])
            gradient_checkpointing = gr.Checkbox(label=get_text('gradient_checkpointing'), value=default_config["gradient_checkpointing"])
        with gr.Row():
            optimizer = gr.Dropdown(label=get_text('optimizer'), value=default_config["optimizer"], choices=["adamw","prodigy"])
            lr_scheduler = gr.Dropdown(label=get_text('lr_scheduler'), value=default_config["lr_scheduler"], 
                        choices=["linear", "cosine", "cosine_with_restarts", "polynomial","constant", "constant_with_warmup"])
            cosine_restarts = gr.Number(label=get_text('cosine_restarts'), value=default_config["cosine_restarts"], info=get_text('cosine_restarts_info'), minimum=1)
        with gr.Row():
            learning_rate = gr.Number(label=get_text('learning_rate'), value=default_config["learning_rate"], info=get_text('learning_rate_info'))
            lr_warmup_steps = gr.Number(label=get_text('lr_warmup_steps'), value=default_config["lr_warmup_steps"])
            seed = gr.Number(label=get_text('seed'), value=default_config["seed"])
        with gr.Row():
            blocks_to_swap = gr.Number(label=get_text('blocks_to_swap'), value=default_config["blocks_to_swap"], info=get_text('blocks_to_swap_info'))
            mask_dropout = gr.Number(label=get_text('mask_dropout'), value=default_config["mask_dropout"], info=get_text('mask_dropout_info'))
            reg_ratio = gr.Number(label=get_text('reg_ratio'), value=default_config["reg_ratio"], info=get_text('reg_ratio_info'))
            reg_timestep = gr.Number(label=get_text('reg_timestep'), value=default_config["reg_timestep"], info=get_text('reg_timestep_info'))
            
            
    misc_accordion = gr.Accordion(get_text('misc'))
    with misc_accordion:
        with gr.Row():
            num_train_epochs = gr.Number(label=get_text('num_train_epochs'), value=default_config["num_train_epochs"], info=get_text('num_train_epochs_info'))
            save_model_epochs = gr.Number(label=get_text('save_model_epochs'), value=default_config["save_model_epochs"], info=get_text('save_model_epochs_info'))
            validation_epochs = gr.Number(label=get_text('validation_epochs'), value=default_config["validation_epochs"], info=get_text('validation_epochs_info'))
        with gr.Row():
            skip_epoch = gr.Number(label=get_text('skip_epoch'), value=default_config["skip_epoch"], info=get_text('skip_epoch_info'))
            skip_step = gr.Number(label=get_text('skip_step'), value=default_config["skip_step"], info=get_text('skip_step_info'))
            validation_ratio = gr.Number(label=get_text('validation_ratio'), value=default_config["validation_ratio"], info=get_text('validation_ratio_info'))
            
        with gr.Row():
            recreate_cache = gr.Checkbox(label=get_text('recreate_cache'), value=default_config["recreate_cache"])
            caption_dropout = gr.Number(label=get_text('caption_dropout'), value=default_config["caption_dropout"], info=get_text('caption_dropout_info'), maximum=1, minimum=0)
            max_time_steps = gr.Number(label=get_text('max_time_steps'), value=default_config["max_time_steps"], info=get_text('max_time_steps_info'), maximum=1000, minimum=0)
        
        resolution_md = gr.Markdown(get_text('resolution_section'))
        with gr.Row():
            resolution = gr.Dropdown(label=get_text('resolution'), value=default_config["resolution"], choices=default_config["resolution_choices"])
    
    misc_accordion = gr.Accordion(get_text('slider'))
    with misc_accordion:
        with gr.Row():
            use_two_captions = gr.Checkbox(label=get_text('use_two_captions'), value=default_config["use_two_captions"])
            slider_positive_scale = gr.Number(label=get_text('slider_positive_scale'), value=default_config["slider_positive_scale"])
            slider_negative_scale = gr.Number(label=get_text('slider_negative_scale'), value=default_config["slider_negative_scale"])
       
    
    
    # è¾“å‡ºå’Œè¿è¡ŒæŒ‰é’®
    output = gr.Textbox(label=get_text('output_box'))
    run_btn = gr.Button(get_text('run'))
    
    # å®šä¹‰æ‰€æœ‰è¾“å…¥ç»„ä»¶åˆ—è¡¨
    inputs = [
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        algo,
        conv_dim,
        conv_alpha,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep,
        use_two_captions,
        slider_positive_scale,
        slider_negative_scale
        # freeze_transformer_layers,
    ]
    
    # è¯­è¨€åˆ‡æ¢å¤„ç†å‡½æ•°
    def update_language_interface():
        """æ›´æ–°ç•Œé¢è¯­è¨€ï¼Œè¿”å›æ‰€æœ‰éœ€è¦æ›´æ–°çš„ç»„ä»¶"""
        toggle_language()
        # è¿”å›æ›´æ–°åçš„æ‰€æœ‰UIç»„ä»¶
        updated_components = [
            # åŸºç¡€ç»„ä»¶
            gr.Markdown(get_text('title')),  # æ ‡é¢˜
            gr.Button(get_text('language_toggle'), scale=0, size="sm"),  # è¯­è¨€åˆ‡æ¢æŒ‰é’®
            gr.Dropdown(label=get_text('script'), value=default_config["script"], choices=default_config["script_choices"]),  # è„šæœ¬é€‰æ‹©
            gr.Textbox(scale=3, label=get_text('config_path'), value=default_config["config_path"], placeholder=get_text('config_path_placeholder')),  # é…ç½®è·¯å¾„
            gr.Button(get_text('save'), scale=1),  # ä¿å­˜æŒ‰é’®
            gr.Button(get_text('load'), scale=1),  # åŠ è½½æŒ‰é’®
            
            # Accordionç»„ä»¶æ›´æ–°
            gr.Accordion(get_text('directory_section')),  # ç›®å½•é…ç½®æ ‡é¢˜
            gr.Accordion(get_text('lora_config')),  # LoRAè®¾ç½®æ ‡é¢˜
            gr.Accordion(get_text('misc')),  # æ‚é¡¹æ ‡é¢˜
            
            # ç›®å½•è®¾ç½®éƒ¨åˆ†çš„ç»„ä»¶
            gr.Textbox(label=get_text('output_dir'), value=default_config["output_dir"], placeholder=get_text('output_dir_placeholder')),  # è¾“å‡ºç›®å½•
            gr.Textbox(label=get_text('save_name'), value=default_config["save_name"], placeholder=get_text('save_name_placeholder')),  # ä¿å­˜åç§°
            gr.Textbox(label=get_text('pretrained_model_name_or_path'), value=default_config["pretrained_model_name_or_path"], placeholder=get_text('pretrained_model_name_or_path_placeholder')),  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            gr.Textbox(label=get_text('resume_from_checkpoint'), value=default_config["resume_from_checkpoint"], placeholder=get_text('resume_from_checkpoint_placeholder')),  # æ¢å¤æ£€æŸ¥ç‚¹
            gr.Textbox(label=get_text('train_data_dir'), value=default_config["train_data_dir"], placeholder=get_text('train_data_dir_placeholder')),  # è®­ç»ƒæ•°æ®ç›®å½•
            gr.Textbox(label=get_text('model_path'), value=default_config["model_path"], placeholder=get_text('model_path_placeholder')),  # æ¨¡å‹è·¯å¾„
            gr.Dropdown(label=get_text('report_to'), value=default_config["report_to"], choices=["all","wandb","tensorboard"]),  # æŠ¥å‘Šåˆ°
            
            # LoRAé…ç½®éƒ¨åˆ†çš„ç»„ä»¶
            gr.Number(label=get_text('rank'), value=default_config["rank"], info=get_text('rank_info')),  # æ’å
            gr.Textbox(label=get_text('algo'), value=default_config["algo"]),
            gr.Number(label=get_text('conv_dim'), value=default_config["conv_dim"]),
            gr.Number(label=get_text('conv_alpha'), value=default_config["conv_alpha"]),
            gr.Number(label=get_text('train_batch_size'), value=default_config["train_batch_size"], info=get_text('train_batch_size_info')),  # è®­ç»ƒæ‰¹æ¬¡å¤§å°
            gr.Number(label=get_text('repeats'), value=default_config["repeats"]),  # é‡å¤æ¬¡æ•°
            gr.Number(label=get_text('gradient_accumulation_steps'), value=default_config["gradient_accumulation_steps"]),  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            gr.Radio(label=get_text('mixed_precision'), value=default_config["mixed_precision"], choices=["bf16", "fp8"]),  # æ··åˆç²¾åº¦
            gr.Checkbox(label=get_text('gradient_checkpointing'), value=default_config["gradient_checkpointing"]),  # æ¢¯åº¦æ£€æŸ¥ç‚¹
            gr.Dropdown(label=get_text('optimizer'), value=default_config["optimizer"], choices=["adamw","prodigy"]),  # ä¼˜åŒ–å™¨
            gr.Dropdown(label=get_text('lr_scheduler'), value=default_config["lr_scheduler"], choices=["linear", "cosine", "cosine_with_restarts", "polynomial","constant", "constant_with_warmup"]),  # å­¦ä¹ ç‡è°ƒåº¦å™¨
            gr.Number(label=get_text('cosine_restarts'), value=default_config["cosine_restarts"], info=get_text('cosine_restarts_info'), minimum=1),  # ä½™å¼¦é‡å¯
            gr.Number(label=get_text('learning_rate'), value=default_config["learning_rate"], info=get_text('learning_rate_info')),  # å­¦ä¹ ç‡
            gr.Number(label=get_text('lr_warmup_steps'), value=default_config["lr_warmup_steps"]),  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
            gr.Number(label=get_text('seed'), value=default_config["seed"]),  # éšæœºç§å­
            gr.Number(label=get_text('blocks_to_swap'), value=default_config["blocks_to_swap"], info=get_text('blocks_to_swap_info')),  # äº¤æ¢å—æ•°
            gr.Number(label=get_text('mask_dropout'), value=default_config["mask_dropout"], info=get_text('mask_dropout_info')),  # æ©ç ä¸¢å¼ƒ
            gr.Number(label=get_text('reg_ratio'), value=default_config["reg_ratio"], info=get_text('reg_ratio_info')),  # æ­£åˆ™åŒ–æ¯”ç‡
            gr.Number(label=get_text('reg_timestep'), value=default_config["reg_timestep"], info=get_text('reg_timestep_info')),  # æ­£åˆ™åŒ–æ—¶é—´æ­¥
            
            # Miscéƒ¨åˆ†çš„ç»„ä»¶
            gr.Number(label=get_text('num_train_epochs'), value=default_config["num_train_epochs"], info=get_text('num_train_epochs_info')),  # è®­ç»ƒè½®æ•°
            gr.Number(label=get_text('save_model_epochs'), value=default_config["save_model_epochs"], info=get_text('save_model_epochs_info')),  # ä¿å­˜æ¨¡å‹è½®æ•°
            gr.Number(label=get_text('validation_epochs'), value=default_config["validation_epochs"], info=get_text('validation_epochs_info')),  # éªŒè¯è½®æ•°
            gr.Number(label=get_text('skip_epoch'), value=default_config["skip_epoch"], info=get_text('skip_epoch_info')),  # è·³è¿‡è½®æ•°
            gr.Number(label=get_text('skip_step'), value=default_config["skip_step"], info=get_text('skip_step_info')),  # è·³è¿‡æ­¥æ•°
            gr.Number(label=get_text('validation_ratio'), value=default_config["validation_ratio"], info=get_text('validation_ratio_info')),  # éªŒè¯æ¯”ç‡
            gr.Checkbox(label=get_text('recreate_cache'), value=default_config["recreate_cache"]),  # é‡å»ºç¼“å­˜
            gr.Number(label=get_text('caption_dropout'), value=default_config["caption_dropout"], info=get_text('caption_dropout_info'), maximum=1, minimum=0),  # æ ‡é¢˜ä¸¢å¼ƒ
            gr.Number(label=get_text('max_time_steps'), value=default_config["max_time_steps"], info=get_text('max_time_steps_info'), maximum=1000, minimum=0),  # æœ€å¤§æ—¶é—´æ­¥
            gr.Markdown(get_text('resolution_section')),  # åˆ†è¾¨ç‡è¯´æ˜
            gr.Dropdown(label=get_text('resolution'), value=default_config["resolution"], choices=default_config["resolution_choices"]),  # åˆ†è¾¨ç‡
            
            # è¾“å‡ºå’Œè¿è¡ŒæŒ‰é’®
            gr.Textbox(label=get_text('output_box')),  # è¾“å‡ºæ¡†
            gr.Button(get_text('run_button')),  # è¿è¡ŒæŒ‰é’®
            
            gr.Checkbox(label=get_text('use_two_captions'), value=default_config["use_two_captions"]),
            gr.Number(label=get_text('slider_positive_scale'), value=default_config["slider_positive_scale"]),
            gr.Number(label=get_text('slider_negative_scale'), value=default_config["slider_negative_scale"])
       
    
        ]
        return updated_components
    
    # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
    run_btn.click(fn=run, inputs=inputs, outputs=output, api_name="run")
    save_config_btn.click(fn=save_config, inputs=inputs)
    load_config_btn.click(fn=load_config, inputs=[config_path], outputs=inputs)
    
    # è¯­è¨€åˆ‡æ¢äº‹ä»¶å¤„ç† - æ›´æ–°æ‰€æœ‰ç»„ä»¶
    language_toggle_btn.click(
        fn=update_language_interface,
        inputs=[],
        outputs=[
            title_md, language_toggle_btn, script, config_path, save_config_btn, load_config_btn,
            directory_accordion, lora_accordion, misc_accordion,  # æ·»åŠ Accordionç»„ä»¶
            output_dir, save_name, pretrained_model_name_or_path, resume_from_checkpoint, 
            train_data_dir, model_path, report_to,
            rank, algo, conv_dim, conv_alpha, train_batch_size, repeats, gradient_accumulation_steps, mixed_precision, gradient_checkpointing,
            optimizer, lr_scheduler, cosine_restarts, learning_rate, lr_warmup_steps, seed,
            blocks_to_swap, mask_dropout, reg_ratio, reg_timestep,
            num_train_epochs, save_model_epochs, validation_epochs, skip_epoch, skip_step, validation_ratio,
            recreate_cache, caption_dropout, max_time_steps, resolution_md, resolution,
            output, run_btn,
            use_two_captions, slider_positive_scale,slider_negative_scale
        ]
    )

# å¯åŠ¨ç•Œé¢
if __name__ == "__main__":
    demo.launch()