import gradio as gr
import subprocess
import json
import sys
import os
import glob

# ===== è¯­è¨€ç¿»è¯‘ç³»ç»Ÿ =====
TRANSLATIONS = {
    'zh': {
        'title': '## LoRA è®­ç»ƒ',
        'script': 'è®­ç»ƒè„šæœ¬',
        'config_path': 'é…ç½®æ–‡ä»¶è·¯å¾„ (.jsonæ–‡ä»¶)',
        'config_path_placeholder': 'è¾“å…¥ä¿å­˜/åŠ è½½é…ç½®çš„è·¯å¾„',
        'save': 'ä¿å­˜',
        'load': 'åŠ è½½',
        'directory_section': 'ç›®å½•é…ç½®',
        'output_dir': 'è¾“å‡ºç›®å½•',
        'output_dir_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®',
        'save_name': 'ä¿å­˜åç§°',
        'save_name_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜åç§°',
        'pretrained_model_name_or_path': 'é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„',
        'pretrained_model_placeholder': 'ä»“åº“åç§°æˆ–åŒ…å«diffusersæ¨¡å‹ç»“æ„çš„ç›®å½•',
        'resume_from_checkpoint': 'ä»æ£€æŸ¥ç‚¹æ¢å¤',
        'resume_checkpoint_placeholder': 'ä»é€‰å®šç›®å½•æ¢å¤loraæƒé‡',
        'train_data_dir': 'è®­ç»ƒæ•°æ®ç›®å½•',
        'train_data_dir_placeholder': 'åŒ…å«æ•°æ®é›†çš„ç›®å½•',
        'model_path': 'æ¨¡å‹è·¯å¾„',
        'model_path_placeholder': 'å¦‚æœä¸æ˜¯ä»å®˜æ–¹æƒé‡è®­ç»ƒåˆ™ä¸ºå•ä¸ªæƒé‡æ–‡ä»¶',
        'report_to': 'æŠ¥å‘Šåˆ°',
        'lora_config': 'LoRA é…ç½®',
        'rank': 'ç§©',
        'rank_info': 'å»ºè®®å¯¹å°äº100çš„è®­ç»ƒé›†ä½¿ç”¨ç§©4',
        'train_batch_size': 'è®­ç»ƒæ‰¹æ¬¡å¤§å°',
        'batch_size_info': 'æ‰¹æ¬¡å¤§å°1ä½¿ç”¨18GBã€‚è¯·ä½¿ç”¨å°æ‰¹æ¬¡å¤§å°ä»¥é¿å…å†…å­˜ä¸è¶³',
        'repeats': 'é‡å¤æ¬¡æ•°',
        'gradient_accumulation_steps': 'æ¢¯åº¦ç´¯ç§¯æ­¥æ•°',
        'mixed_precision': 'æ··åˆç²¾åº¦',
        'gradient_checkpointing': 'æ¢¯åº¦æ£€æŸ¥ç‚¹',
        'optimizer': 'ä¼˜åŒ–å™¨',
        'lr_scheduler': 'å­¦ä¹ ç‡è°ƒåº¦å™¨',
        'cosine_restarts': 'ä½™å¼¦é‡å¯',
        'cosine_restarts_info': 'ä»…å¯¹å­¦ä¹ ç‡è°ƒåº¦å™¨cosine_with_restartsæœ‰ç”¨',
        'learning_rate': 'å­¦ä¹ ç‡',
        'learning_rate_info': 'æ¨èï¼š1e-4 æˆ– prodigyä½¿ç”¨1',
        'lr_warmup_steps': 'å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°',
        'seed': 'éšæœºç§å­',
        'blocks_to_swap': 'äº¤æ¢å—æ•°',
        'blocks_to_swap_info': 'äº¤æ¢åˆ°CPUçš„å—æ•°ã€‚å»ºè®®24GBä½¿ç”¨10ï¼Œæ›´ä½æ˜¾å­˜ä½¿ç”¨æ›´å¤š',
        'mask_dropout': 'æ©ç ä¸¢å¼ƒ',
        'mask_dropout_info': 'ä¸¢å¼ƒæ©ç ï¼Œæ„å‘³ç€æ•´ä¸ªå›¾åƒé‡å»ºçš„æ©ç å…¨ä¸º1',
        'reg_ratio': 'æ­£åˆ™åŒ–æ¯”ç‡',
        'reg_ratio_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º1',
        'reg_timestep': 'æ­£åˆ™åŒ–æ—¶é—´æ­¥',
        'reg_timestep_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º0',
        'misc': 'æ‚é¡¹',
        'num_train_epochs': 'è®­ç»ƒè½®æ•°',
        'num_train_epochs_info': 'è®­ç»ƒçš„æ€»è½®æ•°',
        'save_model_epochs': 'ä¿å­˜æ¨¡å‹è½®æ•°',
        'save_model_epochs_info': 'æ¯xè½®ä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_epochs': 'éªŒè¯è½®æ•°',
        'validation_epochs_info': 'æ¯xè½®æ‰§è¡ŒéªŒè¯',
        'skip_epoch': 'è·³è¿‡è½®æ•°',
        'skip_epoch_info': 'è·³è¿‡xè½®è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'skip_step': 'è·³è¿‡æ­¥æ•°',
        'skip_step_info': 'è·³è¿‡xæ­¥è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_ratio': 'éªŒè¯æ¯”ä¾‹',
        'validation_ratio_info': 'æŒ‰æ­¤æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†ç”¨äºéªŒè¯',
        'recreate_cache': 'é‡æ–°åˆ›å»ºç¼“å­˜',
        'caption_dropout': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'caption_dropout_info': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'max_time_steps': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        'max_time_steps_info': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        # 'resolution_section': '## å®éªŒé€‰é¡¹ï¼šåˆ†è¾¨ç‡\n- åŸºäºç›®æ ‡åˆ†è¾¨ç‡ï¼ˆé»˜è®¤ï¼š1024ï¼‰ã€‚\n- æ”¯æŒ512æˆ–1024ã€‚',
        'resolution': 'åˆ†è¾¨ç‡',
        'output_box': 'è¾“å‡ºæ¡†',
        'run': 'è¿è¡Œ',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°English',
        'slider': 'æ»‘å—è®­ç»ƒç›¸å…³',
        'use_two_captions': 'ä½¿ç”¨ä¸¤å¯¹åº”æ–‡æœ¬æ ‡æ³¨',
        'slider_positive_scale': 'æ»‘å—æ­£å‘ç›®æ ‡å¼ºåº¦',
        'slider_negative_scale': 'æ»‘å—è´Ÿé¢ç›®æ ‡å¼ºåº¦',
        'config_template': 'è®­ç»ƒæ’ç‰ˆé…ç½®æ¨¡æ¿(Kontext_new & qwen image)',
        'select_template': 'é€‰æ‹©é…ç½®æ¨¡æ¿',
        'save_edited_json': 'ä¿å­˜ç¼–è¾‘åçš„JSONåˆ°Config'
    },
    'en': {
        'title': '## Lora Training',
        'script': 'script',
        'config_path': 'Config Path (.json file)',
        'config_path_placeholder': 'Enter path to save/load config',
        'save': 'Save',
        'load': 'load',
        'directory_section': 'Directory section',
        'output_dir': 'output_dir',
        'output_dir_placeholder': 'checkpoint save to',
        'save_name': 'save_name',
        'save_name_placeholder': 'checkpoint save name',
        'pretrained_model_name_or_path': 'pretrained_model_name_or_path',
        'pretrained_model_placeholder': 'repo name or dir contains diffusers model structure',
        'resume_from_checkpoint': 'resume_from_checkpoint',
        'resume_checkpoint_placeholder': 'resume the lora weight from seleted dir',
        'train_data_dir': 'train_data_dir',
        'train_data_dir_placeholder': 'dir contains dataset',
        'model_path': 'model_path',
        'model_path_placeholder': 'single weight files if not trained from official weight',
        'report_to': 'report_to',
        'lora_config': 'Lora Config',
        'rank': 'rank',
        'rank_info': 'Recommanded to use rank 4 for training set small than 100.',
        'train_batch_size': 'train_batch_size',
        'batch_size_info': 'Batch size 1 is using 18GB. Please use small batch size to avoid oom.',
        'repeats': 'repeats',
        'gradient_accumulation_steps': 'gradient_accumulation_steps',
        'mixed_precision': 'mixed_precision',
        'gradient_checkpointing': 'gradient_checkpointing',
        'optimizer': 'optimizer',
        'lr_scheduler': 'lr_scheduler',
        'cosine_restarts': 'cosine_restarts',
        'cosine_restarts_info': 'Only useful for lr_scheduler: cosine_with_restarts',
        'learning_rate': 'learning_rate',
        'learning_rate_info': 'Recommended: 1e-4 or 1 for prodigy',
        'lr_warmup_steps': 'lr_warmup_steps',
        'seed': 'seed',
        'blocks_to_swap': 'blocks_to_swap',
        'blocks_to_swap_info': 'How many blocks to swap to cpu. It is suggested 10 for 24 GB and more for lower VRAM',
        'mask_dropout': 'mask_dropout',
        'mask_dropout_info': 'Dropout mask which means mask is all one for whole image reconstruction',
        'reg_ratio': 'reg_ratio',
        'reg_ratio_info': 'As regularization of objective transfer learning. Set as 1 if you aren\'t training different objective.',
        'reg_timestep': 'reg_timestep',
        'reg_timestep_info': 'As regularization of objective transfer learning. Set as 0 if you aren\'t training different objective.',
        'misc': 'Misc',
        'num_train_epochs': 'num_train_epochs',
        'num_train_epochs_info': 'Total epoches of the training',
        'save_model_epochs': 'save_model_epochs',
        'save_model_epochs_info': 'Save checkpoint when x epoches',
        'validation_epochs': 'validation_epochs',
        'validation_epochs_info': 'perform validation when x epoches',
        'skip_epoch': 'skip_epoch',
        'skip_epoch_info': 'Skip x epoches for validation and save checkpoint',
        'skip_step': 'skip_step',
        'skip_step_info': 'Skip x steps for validation and save checkpoint',
        'validation_ratio': 'validation_ratio',
        'validation_ratio_info': 'Split dataset with this ratio for validation',
        'recreate_cache': 'recreate_cache',
        'caption_dropout': 'Caption Dropout',
        'caption_dropout_info': 'Caption Dropout',
        'max_time_steps': 'Max timesteps limitation',
        'max_time_steps_info': 'Max timesteps limitation',
        # 'resolution_section': '## Experiment Option: resolution\n- Based target resolution (default:1024). \n- 512 or 1024 are supported.',
        'resolution': 'resolution',
        'output_box': 'Output Box',
        'run': 'Run',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°ä¸­æ–‡',
        'slider': 'Slider Related',
        'use_two_captions': 'Use two captions for each direction',
        'slider_positive_scale': 'Slider positive scale',
        'slider_negative_scale': 'Slider negative scale',
        'config_template': 'Train Layout Config Template (Kontext_new & qwen image)',
        'select_template': 'Select config template',
        'save_edited_json': 'Save Edited JSON to Config'
    }
}

current_language = 'en'

def update_labels_only():
    toggle_language()
    return {
        title_md: gr.update(value=get_text('title')),
        script: gr.update(label=get_text('script')),
        config_path: gr.update(label=get_text('config_path'), placeholder=get_text('config_path_placeholder')),
        save_config_btn: gr.update(value=get_text('save')),
        load_config_btn: gr.update(value=get_text('load')),
        output_dir: gr.update(label=get_text('output_dir'), placeholder=get_text('output_dir_placeholder')),
        save_name: gr.update(label=get_text('save_name'), placeholder=get_text('save_name_placeholder')),
        pretrained_model_name_or_path: gr.update(label=get_text('pretrained_model_name_or_path'), placeholder=get_text('pretrained_model_placeholder')),
        resume_from_checkpoint: gr.update(label=get_text('resume_from_checkpoint'), placeholder=get_text('resume_checkpoint_placeholder')),
        train_data_dir: gr.update(label=get_text('train_data_dir'), placeholder=get_text('train_data_dir_placeholder')),
        model_path: gr.update(label=get_text('model_path'), placeholder=get_text('model_path_placeholder')),
        report_to: gr.update(label=get_text('report_to')),
        rank: gr.update(label=get_text('rank'), info=get_text('rank_info')),
        train_batch_size: gr.update(label=get_text('train_batch_size'), info=get_text('batch_size_info')),
        repeats: gr.update(label=get_text('repeats')),
        gradient_accumulation_steps: gr.update(label=get_text('gradient_accumulation_steps')),
        mixed_precision: gr.update(label=get_text('mixed_precision')),
        gradient_checkpointing: gr.update(label=get_text('gradient_checkpointing')),
        optimizer: gr.update(label=get_text('optimizer')),
        lr_scheduler: gr.update(label=get_text('lr_scheduler')),
        cosine_restarts: gr.update(label=get_text('cosine_restarts'), info=get_text('cosine_restarts_info')),
        learning_rate: gr.update(label=get_text('learning_rate'), info=get_text('learning_rate_info')),
        lr_warmup_steps: gr.update(label=get_text('lr_warmup_steps')),
        seed: gr.update(label=get_text('seed')),
        blocks_to_swap: gr.update(label=get_text('blocks_to_swap'), info=get_text('blocks_to_swap_info')),
        mask_dropout: gr.update(label=get_text('mask_dropout'), info=get_text('mask_dropout_info')),
        reg_ratio: gr.update(label=get_text('reg_ratio'), info=get_text('reg_ratio_info')),
        reg_timestep: gr.update(label=get_text('reg_timestep'), info=get_text('reg_timestep_info')),
        num_train_epochs: gr.update(label=get_text('num_train_epochs'), info=get_text('num_train_epochs_info')),
        save_model_epochs: gr.update(label=get_text('save_model_epochs'), info=get_text('save_model_epochs_info')),
        validation_epochs: gr.update(label=get_text('validation_epochs'), info=get_text('validation_epochs_info')),
        skip_epoch: gr.update(label=get_text('skip_epoch'), info=get_text('skip_epoch_info')),
        skip_step: gr.update(label=get_text('skip_step'), info=get_text('skip_step_info')),
        validation_ratio: gr.update(label=get_text('validation_ratio'), info=get_text('validation_ratio_info')),
        recreate_cache: gr.update(label=get_text('recreate_cache')),
        caption_dropout: gr.update(label=get_text('caption_dropout'), info=get_text('caption_dropout_info')),
        max_time_steps: gr.update(label=get_text('max_time_steps'), info=get_text('max_time_steps_info')),
        resolution: gr.update(label=get_text('resolution')),
        use_two_captions: gr.update(label=get_text('use_two_captions')),
        slider_positive_scale: gr.update(label=get_text('slider_positive_scale')),
        slider_negative_scale: gr.update(label=get_text('slider_negative_scale')),
        template_dropdown: gr.update(label=get_text('select_template')),
        # save_edited_json_btn: gr.update(value=get_text('save_edited_json')),
        language_toggle_btn: gr.update(value=get_text('language_toggle')),
        output: gr.update(label=get_text('output_box')),
        run_btn: gr.update(value=get_text('run')),
        
        save_config_btn2: gr.update(value=get_text('save')),
        load_config_btn2: gr.update(value=get_text('load')),
    }

def get_text(key):
    return TRANSLATIONS[current_language].get(key, key)

def toggle_language():
    global current_language
    current_language = 'en' if current_language == 'zh' else 'zh'

TEMPLATE_DIR = "./config_template"
os.makedirs(TEMPLATE_DIR, exist_ok=True)

def list_templates(config_path):
    # return [os.path.basename(f) for f in glob.glob(os.path.join(TEMPLATE_DIR, "*.json"))]
    templates = [os.path.basename(f) for f in glob.glob(os.path.join(TEMPLATE_DIR, "*.json"))]
    templates.sort()
    # æŠŠ config_path ä¹ŸåŠ è¿›å»ï¼ˆåªä¿ç•™æ–‡ä»¶åï¼Œä¾¿äºæ˜¾ç¤ºï¼‰
    config_name = os.path.basename(config_path) if os.path.isfile(config_path) else "config.json"
    choices = [config_name] + templates
    return choices
# ------------------------------------------------------------------
#  Script-dependent template injection
# ------------------------------------------------------------------
def change_script(selected_script, config_path):
    """
    Called when the script dropdown changes.
    If the chosen script is 'train_qwen_image.py' we ensure that
    'image_configs', 'caption_configs', and 'training_set' exist
    by merging them from preset_0_single.json when they are absent.
    The merged (or original) config is then returned for display.
    """
    if selected_script != "train_qwen_image.py" and selected_script != "train_flux_lora_ui_kontext_new.py":
        return ""
    preset_name = "preset_0_single.json"
    preset_path = os.path.join(TEMPLATE_DIR, preset_name)

    editor = {}
    preset = {}
    mandatory = {"image_configs", "caption_configs", "training_set"}
    if os.path.isfile(config_path):
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                preset = json.load(f)
        except Exception:
            pass  # ignore read/parse errors
    else:
        try:
            with open(preset_path, "r", encoding="utf-8") as f:
                preset = json.load(f)
        except Exception:
            # Preset unreadable â†’ return current config
            # return json.dumps(current, indent=4, ensure_ascii=False)
            print(f"Error reading {preset_path}")

    # 5. Merge missing mandatory sections from preset
    for key in mandatory:
        if key in preset:
            editor[key] = preset[key]

    # 6. Return the merged JSON for display
    return json.dumps(editor, indent=4, ensure_ascii=False)

def load_template(template_name, config_path):
    """
    ä»…å½“ config_path ä¸­ç¼ºå°‘ image_configs / caption_configs / training_set æ—¶ï¼Œ
    æ‰ç”¨æ¨¡æ¿é‡Œçš„å¯¹åº”é”®è¡¥å……ï¼Œå…¶ä½™ä¿æŒä¸å˜ã€‚
    """
    if isinstance(template_name, list):
        template_name = template_name[0] if template_name else ""
    if not template_name:
        return ""

    tpl = {}
    config_name = os.path.basename(config_path)
    if template_name == config_name and os.path.isfile(config_path):
        # é€‰çš„æ˜¯å½“å‰ config
        with open(config_path, "r", encoding="utf-8") as f:
            tpl = json.load(f)
    else:
        # é€‰çš„æ˜¯æ¨¡æ¿
        tpl_path = os.path.join(TEMPLATE_DIR, template_name)
        if not os.path.isfile(tpl_path):
            return ""

        with open(tpl_path, "r", encoding="utf-8") as f:
            tpl = json.load(f)
        
    current = {}

    # ä»…è¡¥å……ç¼ºå¤±çš„ä¸‰é¡¹
    for key in ("image_configs", "caption_configs", "training_set"):
        if key in tpl and key not in current:
            current[key] = tpl[key]

    return json.dumps(current, indent=4, ensure_ascii=False)


default_config = {
    "script": "train_flux_lora_ui_kontext.py",
    "script_choices": [
        "train_qwen_image_edit.py",
        "train_qwen_image.py",
        "train_flux_lora_ui_kontext_new.py",
        "train_flux_lora_ui_kontext.py",
        "train_flux_lora_ui_kontext_slider.py",
        "train_flux_lora_ui_with_mask.py",
        "train_flux_lora_ui.py",
    ],
    "output_dir": "/home/waas/kontext_output",
    "save_name": "flux-lora",
    "pretrained_model_name_or_path": "/datasets/T2ITrainer/models/flux-kontext-nf4",
    "train_data_dir": "/home/waas/kontext_test",
    "resume_from_checkpoint": None,
    "model_path": None,
    "report_to": "all",
    "rank": 16,
    "train_batch_size": 1,
    "repeats": 1,
    "gradient_accumulation_steps": 1,
    "mixed_precision": "bf16",
    "gradient_checkpointing": True,
    "optimizer": "adamw",
    "lr_scheduler": "constant",
    "learning_rate": 1e-4,
    "lr_warmup_steps": 0,
    "seed": 4321,
    "num_train_epochs": 5,
    "save_model_epochs": 1,
    "validation_epochs": 1,
    "skip_epoch": 0,
    "skip_step": 0,
    "validation_ratio": 0.1,
    "recreate_cache": False,
    "caption_dropout": 0.1,
    "config_path": "config.json",
    "resolution": "512",
    "resolution_choices": ["1344","1024", "768", "512", "256"],
    "use_debias": False,
    "snr_gamma": 0,
    "cosine_restarts": 1,
    "max_time_steps": 0,
    "blocks_to_swap": 0,
    "mask_dropout": 0,
    "reg_ratio": 0.0,
    "reg_timestep": 0,
    "use_two_captions": False,
    "slider_positive_scale": 1.0,
    "slider_negative_scale": -1.0
}

def save_config(
    config_path, script, seed, mixed_precision, report_to, lr_warmup_steps,
    output_dir, save_name, train_data_dir, optimizer, lr_scheduler, learning_rate,
    train_batch_size, repeats, gradient_accumulation_steps, num_train_epochs,
    save_model_epochs, validation_epochs, rank, skip_epoch, skip_step,
    gradient_checkpointing, validation_ratio, pretrained_model_name_or_path,
    model_path, resume_from_checkpoint, recreate_cache, resolution,
    caption_dropout, cosine_restarts, max_time_steps, blocks_to_swap,
    mask_dropout, reg_ratio, reg_timestep, use_two_captions,
    slider_positive_scale, slider_negative_scale,
    json_editor        # è¿™é‡Œåªæ”¾ json_editor å­—ç¬¦ä¸²
):
    
    # 1ï¸âƒ£ å…ˆè¯»åŸæ–‡ä»¶
    if os.path.isfile(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
    else:
        cfg = {}

    # 2ï¸âƒ£ UI å®æ—¶å€¼è¦†ç›–
    ui_vals = {
        "config_path": config_path,
        "script": script,
        "seed": seed,
        "mixed_precision": mixed_precision,
        "report_to": report_to,
        "lr_warmup_steps": lr_warmup_steps,
        "output_dir": output_dir,
        "save_name": save_name,
        "train_data_dir": train_data_dir,
        "optimizer": optimizer,
        "lr_scheduler": lr_scheduler,
        "learning_rate": learning_rate,
        "train_batch_size": train_batch_size,
        "repeats": repeats,
        "gradient_accumulation_steps": gradient_accumulation_steps,
        "num_train_epochs": num_train_epochs,
        "save_model_epochs": save_model_epochs,
        "validation_epochs": validation_epochs,
        "rank": rank,
        "skip_epoch": skip_epoch,
        "skip_step": skip_step,
        "gradient_checkpointing": gradient_checkpointing,
        "validation_ratio": validation_ratio,
        "pretrained_model_name_or_path": pretrained_model_name_or_path,
        "model_path": model_path,
        "resume_from_checkpoint": resume_from_checkpoint,
        "recreate_cache": recreate_cache,
        "resolution": resolution,
        "caption_dropout": caption_dropout,
        "cosine_restarts": cosine_restarts,
        "max_time_steps": max_time_steps,
        "blocks_to_swap": blocks_to_swap,
        "mask_dropout": mask_dropout,
        "reg_ratio": reg_ratio,
        "reg_timestep": reg_timestep,
        "use_two_captions": use_two_captions,
        "slider_positive_scale": slider_positive_scale,
        "slider_negative_scale": slider_negative_scale,
    }
    cfg.update(ui_vals)

    # 3ï¸âƒ£ json_editor åªåšã€Œç¼ºé”®è¡¥å…¨ã€
    try:
        editor_partial = json.loads(json_editor)
    except json.JSONDecodeError:
        editor_partial = {}
        
    # for k, v in editor_partial.items():
    #     cfg.setdefault(k, v)
    
    for key in ("image_configs", "caption_configs", "training_set"):
        if key in editor_partial:
            cfg[key] = {}
            cfg[key] = editor_partial[key]

    print(f"save config to {config_path}")
    # 4ï¸âƒ£ å†™å›
    os.makedirs(os.path.dirname(config_path) or ".", exist_ok=True)
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(cfg, f, indent=4, ensure_ascii=False)
    # åŒæ—¶ä¿ç•™é»˜è®¤å‰¯æœ¬
    with open("config.json", "w", encoding="utf-8") as f:
        json.dump(cfg, f, indent=4, ensure_ascii=False)

# å¯åŠ¨æ—¶é»˜è®¤åŠ è½½ config
def load_first_template():
    templates = list_templates(default_config["config_path"])
    default_choice = os.path.basename(default_config["config_path"])
    return templates, default_choice, load_template(default_choice, default_config["config_path"])


def load_config(config_path):
    if not os.path.isfile(config_path):
        save_config(**default_config)
    with open(config_path, 'r') as f:
        config = json.load(f)
    for k in config:
        default_config[k] = config[k]
    
    loaded_config = [config.get(k, default_config[k]) for k in [
        "config_path","script","seed","mixed_precision","report_to","lr_warmup_steps",
        "output_dir","save_name","train_data_dir","optimizer","lr_scheduler","learning_rate",
        "train_batch_size","repeats","gradient_accumulation_steps","num_train_epochs",
        "save_model_epochs","validation_epochs","rank","skip_epoch","skip_step",
        "gradient_checkpointing","validation_ratio","pretrained_model_name_or_path",
        "model_path","resume_from_checkpoint","recreate_cache","resolution",
        "caption_dropout","cosine_restarts","max_time_steps","blocks_to_swap",
        "mask_dropout","reg_ratio","reg_timestep","use_two_captions",
        "slider_positive_scale","slider_negative_scale"
    ]]
    
    _, _, first_template = load_first_template()
    
    loaded_config += [first_template]
    
    return loaded_config

load_config("config.json")

def run(
        config_path, script, seed, mixed_precision, report_to, lr_warmup_steps,
        output_dir, save_name, train_data_dir, optimizer, lr_scheduler, learning_rate,
        train_batch_size, repeats, gradient_accumulation_steps, num_train_epochs,
        save_model_epochs, validation_epochs, rank, skip_epoch, skip_step,
        gradient_checkpointing, validation_ratio, pretrained_model_name_or_path,
        model_path, resume_from_checkpoint, recreate_cache, resolution,
        caption_dropout, cosine_restarts, max_time_steps, blocks_to_swap,
        mask_dropout, reg_ratio, reg_timestep, use_two_captions,
        slider_positive_scale, slider_negative_scale,
        json_editor
    ):
    save_config(config_path, script, seed, mixed_precision, report_to, lr_warmup_steps,
        output_dir, save_name, train_data_dir, optimizer, lr_scheduler, learning_rate,
        train_batch_size, repeats, gradient_accumulation_steps, num_train_epochs,
        save_model_epochs, validation_epochs, rank, skip_epoch, skip_step,
        gradient_checkpointing, validation_ratio, pretrained_model_name_or_path,
        model_path, resume_from_checkpoint, recreate_cache, resolution,
        caption_dropout, cosine_restarts, max_time_steps, blocks_to_swap,
        mask_dropout, reg_ratio, reg_timestep, use_two_captions,
        slider_positive_scale, slider_negative_scale,
        json_editor)
    cmd = [sys.executable, script, "--config_path", config_path]
    subprocess.call(cmd)
    return " ".join(cmd)

# def update_language_interface():
#     toggle_language()
#     return [get_text('title'),
#             get_text('language_toggle'),
#             get_text('select_template'),
#             get_text('save_edited_json')] + \
#            [get_text(k) for k in [
#                'script','config_path','config_path_placeholder','save','load',
#                'directory_section','output_dir','output_dir_placeholder',
#                'save_name','save_name_placeholder','pretrained_model_name_or_path',
#                'pretrained_model_placeholder','resume_from_checkpoint',
#                'resume_checkpoint_placeholder','train_data_dir','train_data_dir_placeholder',
#                'model_path','model_path_placeholder','report_to','lora_config','rank',
#                'rank_info','train_batch_size','batch_size_info','repeats',
#                'gradient_accumulation_steps','mixed_precision','gradient_checkpointing',
#                'optimizer','lr_scheduler','cosine_restarts','cosine_restarts_info',
#                'learning_rate','learning_rate_info','lr_warmup_steps','seed',
#                'blocks_to_swap','blocks_to_swap_info','mask_dropout','mask_dropout_info',
#                'reg_ratio','reg_ratio_info','reg_timestep','reg_timestep_info','misc',
#                'num_train_epochs','num_train_epochs_info','save_model_epochs',
#                'save_model_epochs_info','validation_epochs','validation_epochs_info',
#                'skip_epoch','skip_epoch_info','skip_step','skip_step_info',
#                'validation_ratio','validation_ratio_info','recreate_cache',
#                'caption_dropout','caption_dropout_info','max_time_steps',
#                'max_time_steps_info','resolution_section','resolution','output_box','run',
#                'slider','use_two_captions','slider_positive_scale','slider_negative_scale'
#            ]]

with gr.Blocks() as demo:
    with gr.Row():
        gr.HTML("<div style='flex-grow: 1;'></div>")
        language_toggle_btn = gr.Button(get_text('language_toggle'), scale=0, size="sm")
    title_md = gr.Markdown(get_text('title'))

    script = gr.Dropdown(label=get_text('script'), value=default_config["script"], choices=default_config["script_choices"])
    with gr.Row(equal_height=True):
        config_path = gr.Textbox(scale=3, label=get_text('config_path'), value=default_config["config_path"], placeholder=get_text('config_path_placeholder'))
        save_config_btn = gr.Button(get_text('save'), scale=1)
        load_config_btn = gr.Button(get_text('load'), scale=1)

    directory_accordion = gr.Accordion(get_text('directory_section'))
    with directory_accordion:
        with gr.Row():
            output_dir = gr.Textbox(label=get_text('output_dir'), value=default_config["output_dir"], placeholder=get_text('output_dir_placeholder'))
            save_name = gr.Textbox(label=get_text('save_name'), value=default_config["save_name"], placeholder=get_text('save_name_placeholder'))
        with gr.Row():
            pretrained_model_name_or_path = gr.Textbox(label=get_text('pretrained_model_name_or_path'), value=default_config["pretrained_model_name_or_path"], placeholder=get_text('pretrained_model_name_or_path_placeholder'))
            resume_from_checkpoint = gr.Textbox(label=get_text('resume_from_checkpoint'), value=default_config["resume_from_checkpoint"], placeholder=get_text('resume_checkpoint_placeholder'))
        with gr.Row():
            train_data_dir = gr.Textbox(label=get_text('train_data_dir'), value=default_config["train_data_dir"], placeholder=get_text('train_data_dir_placeholder'))
            model_path = gr.Textbox(label=get_text('model_path'), value=default_config["model_path"], placeholder=get_text('model_path_placeholder'))
        with gr.Row():
            report_to = gr.Dropdown(label=get_text('report_to'), value=default_config["report_to"], choices=["all","wandb","tensorboard"])

    lora_accordion = gr.Accordion(get_text('lora_config'))
    with lora_accordion:
        with gr.Row():
            rank = gr.Number(label=get_text('rank'), value=default_config["rank"], info=get_text('rank_info'))
            train_batch_size = gr.Number(label=get_text('train_batch_size'), value=default_config["train_batch_size"], info=get_text('batch_size_info'))
        with gr.Row():
            repeats = gr.Number(label=get_text('repeats'), value=default_config["repeats"])
            gradient_accumulation_steps = gr.Number(label=get_text('gradient_accumulation_steps'), value=default_config["gradient_accumulation_steps"])
            mixed_precision = gr.Radio(label=get_text('mixed_precision'), value=default_config["mixed_precision"], choices=["bf16","fp8"])
            gradient_checkpointing = gr.Checkbox(label=get_text('gradient_checkpointing'), value=default_config["gradient_checkpointing"])
        with gr.Row():
            optimizer = gr.Dropdown(label=get_text('optimizer'), value=default_config["optimizer"], choices=["adamw","prodigy"])
            lr_scheduler = gr.Dropdown(label=get_text('lr_scheduler'), value=default_config["lr_scheduler"], choices=["linear","cosine","cosine_with_restarts","polynomial","constant","constant_with_warmup"])
            cosine_restarts = gr.Number(label=get_text('cosine_restarts'), value=default_config["cosine_restarts"], info=get_text('cosine_restarts_info'), minimum=1)
        with gr.Row():
            learning_rate = gr.Number(label=get_text('learning_rate'), value=default_config["learning_rate"], info=get_text('learning_rate_info'))
            lr_warmup_steps = gr.Number(label=get_text('lr_warmup_steps'), value=default_config["lr_warmup_steps"])
            seed = gr.Number(label=get_text('seed'), value=default_config["seed"])
        with gr.Row():
            blocks_to_swap = gr.Number(label=get_text('blocks_to_swap'), value=default_config["blocks_to_swap"], info=get_text('blocks_to_swap_info'))
            mask_dropout = gr.Number(label=get_text('mask_dropout'), value=default_config["mask_dropout"], info=get_text('mask_dropout_info'))
            reg_ratio = gr.Number(label=get_text('reg_ratio'), value=default_config["reg_ratio"], info=get_text('reg_ratio_info'))
            reg_timestep = gr.Number(label=get_text('reg_timestep'), value=default_config["reg_timestep"], info=get_text('reg_timestep_info'))

    misc_accordion = gr.Accordion(get_text('misc'))
    with misc_accordion:
        with gr.Row():
            num_train_epochs = gr.Number(label=get_text('num_train_epochs'), value=default_config["num_train_epochs"], info=get_text('num_train_epochs_info'))
            save_model_epochs = gr.Number(label=get_text('save_model_epochs'), value=default_config["save_model_epochs"], info=get_text('save_model_epochs_info'))
            validation_epochs = gr.Number(label=get_text('validation_epochs'), value=default_config["validation_epochs"], info=get_text('validation_epochs_info'))
        with gr.Row():
            skip_epoch = gr.Number(label=get_text('skip_epoch'), value=default_config["skip_epoch"], info=get_text('skip_epoch_info'))
            skip_step = gr.Number(label=get_text('skip_step'), value=default_config["skip_step"], info=get_text('skip_step_info'))
            validation_ratio = gr.Number(label=get_text('validation_ratio'), value=default_config["validation_ratio"], info=get_text('validation_ratio_info'))
        with gr.Row():
            recreate_cache = gr.Checkbox(label=get_text('recreate_cache'), value=default_config["recreate_cache"])
            caption_dropout = gr.Number(label=get_text('caption_dropout'), value=default_config["caption_dropout"], info=get_text("caption_dropout_info"), maximum=1, minimum=0)
            max_time_steps = gr.Number(label=get_text('max_time_steps'), value=default_config["max_time_steps"], info=get_text("max_time_steps_info"), maximum=1000, minimum=0)
        # gr.Markdown(get_text('resolution_section'))
        resolution = gr.Dropdown(label=get_text('resolution'), value=default_config["resolution"], choices=default_config["resolution_choices"])

    slider_accordion = gr.Accordion(get_text('slider'))
    with slider_accordion:
        with gr.Row():
            use_two_captions = gr.Checkbox(label=get_text('use_two_captions'), value=default_config["use_two_captions"])
            slider_positive_scale = gr.Number(label=get_text('slider_positive_scale'), value=default_config["slider_positive_scale"])
            slider_negative_scale = gr.Number(label=get_text('slider_negative_scale'), value=default_config["slider_negative_scale"])

    config_template = gr.Accordion(get_text('config_template'))
    with config_template:
        with gr.Row():
            template_dropdown = gr.Dropdown(
                label=get_text('select_template'),
                choices=list_templates(default_config["config_path"]),
                value=os.path.basename(default_config["config_path"]),
                scale=2,
                allow_custom_value=True
            )
        with gr.Row():
            json_editor = gr.Code(language="json", label="Edit JSON Template", lines=25, interactive=True)
        with gr.Row():
            save_config_btn2 = gr.Button(get_text('save'), scale=1)
            load_config_btn2 = gr.Button(get_text('load'), scale=1)
    # save_edited_json_btn = gr.Button(get_text('save_edited_json'))

    output = gr.Textbox(label=get_text('output_box'))
    run_btn = gr.Button(get_text('run'))

    inputs = [
        config_path, script, seed, mixed_precision, report_to, lr_warmup_steps,
        output_dir, save_name, train_data_dir, optimizer, lr_scheduler, learning_rate,
        train_batch_size, repeats, gradient_accumulation_steps, num_train_epochs,
        save_model_epochs, validation_epochs, rank, skip_epoch, skip_step,
        gradient_checkpointing, validation_ratio, pretrained_model_name_or_path,
        model_path, resume_from_checkpoint, recreate_cache, resolution,
        caption_dropout, cosine_restarts, max_time_steps, blocks_to_swap,
        mask_dropout, reg_ratio, reg_timestep, use_two_captions,
        slider_positive_scale, slider_negative_scale,
        json_editor  # â† å°±æ˜¯è¿™ä¸€é¡¹
    ]

    # Events
    run_btn.click(fn=run, inputs=inputs, outputs=output)
    save_config_btn.click(fn=save_config, inputs=inputs)
    load_config_btn.click(fn=load_config, inputs=[config_path], outputs=inputs)
    save_config_btn2.click(fn=save_config, inputs=inputs)
    load_config_btn2.click(fn=load_config, inputs=[config_path], outputs=inputs)
    
    script.change(fn=change_script, inputs=[script, config_path], outputs=[json_editor])
    
    template_dropdown.change(
        fn=load_template,
        inputs=[template_dropdown, config_path],
        outputs=[json_editor]
    )
    # save_edited_json_btn.click(
    #     fn=save_edited_json,
    #     inputs=[json_editor, config_path],
    #     outputs=[output]
    # )

    language_toggle_btn.click(
        fn=update_labels_only,
        inputs=[],
        outputs=[title_md, language_toggle_btn, template_dropdown, 
                #  save_edited_json_btn,
                script, config_path, save_config_btn, load_config_btn,
                output_dir, save_name, pretrained_model_name_or_path, resume_from_checkpoint,
                train_data_dir, model_path, report_to,
                rank, train_batch_size, repeats, gradient_accumulation_steps,
                mixed_precision, gradient_checkpointing, optimizer, lr_scheduler,
                cosine_restarts, learning_rate, lr_warmup_steps, seed,
                blocks_to_swap, mask_dropout, reg_ratio, reg_timestep,
                num_train_epochs, save_model_epochs, validation_epochs,
                skip_epoch, skip_step, validation_ratio, recreate_cache,
                caption_dropout, max_time_steps, resolution,
                use_two_captions, slider_positive_scale, slider_negative_scale,
                json_editor, output, run_btn,
                save_config_btn2, load_config_btn2  # â† åŠ ä¸Šè¿™ä¸¤è¡Œ
                ]
    )

    demo.load(
        fn=lambda: load_first_template(),
        outputs=[template_dropdown, template_dropdown, json_editor]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0")